{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 1**:\n",
    "- This cell imports necessary modules (`tweepy`, `json`, and `os`) and sets up authentication for the Twitter API using credentials stored in a `config.json` file. It ensures the path to the configuration file is correctly constructed and attempts to authenticate to the Twitter API, printing a confirmation message if successful or an error message if authentication fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Correct path to access config.json\n",
    "config_path = os.path.join(os.path.dirname(os.getcwd()), 'config.json')\n",
    "\n",
    "# Load the API credentials from config.json using the corrected path\n",
    "with open(config_path, 'r') as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "# Authenticate to the Twitter API\n",
    "auth = tweepy.OAuthHandler(config['api_key'], config['api_secret_key'])\n",
    "auth.set_access_token(config['access_token'], config['access_token_secret'])\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Check for authentication success\n",
    "try:\n",
    "    api.verify_credentials()\n",
    "    print(\"Authentication OK\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to authenticate:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 2**:\n",
    "- This cell defines a function `print_json_structure` that recursively prints the keys and sub-keys of a JSON-like dictionary or list to visualize its structure. It loads JSON data from a file (`tweets_data.json`) and uses this function to print the structure of the data, helping to understand the hierarchical organization of the JSON content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of 1294 items\n",
      "    public_metrics\n",
      "        retweet_count\n",
      "        reply_count\n",
      "        like_count\n",
      "        quote_count\n",
      "        bookmark_count\n",
      "        impression_count\n",
      "    id\n",
      "    text\n",
      "    conversation_id\n",
      "    edit_history_tweet_ids\n",
      "        List of 1 items\n",
      "    lang\n",
      "    referenced_tweets\n",
      "        List of 1 items\n",
      "            type\n",
      "            id\n",
      "    author_id\n",
      "    context_annotations\n",
      "        List of 6 items\n",
      "            domain\n",
      "                id\n",
      "                name\n",
      "                description\n",
      "            entity\n",
      "                id\n",
      "                name\n",
      "                description\n",
      "    created_at\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def print_json_structure(data, indent=''):\n",
    "    \"\"\"\n",
    "    Recursively prints the keys and sub-keys of a JSON-like dictionary or list.\n",
    "    Args:\n",
    "        data (dict or list): The JSON-like dictionary or list.\n",
    "        indent (str): The indentation string to visualize hierarchy.\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            print(f\"{indent}{key}\")\n",
    "            # Recursively print the structure of the nested dictionary or list\n",
    "            print_json_structure(value, indent + '    ')\n",
    "    elif isinstance(data, list):\n",
    "        print(f\"{indent}List of {len(data)} items\")\n",
    "        # Optionally, you can inspect the first item more closely if all items are expected to be similar\n",
    "        if data:\n",
    "            print_json_structure(data[0], indent + '    ')\n",
    "\n",
    "# Load your JSON data\n",
    "file_path = 'tweets_data.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Print the structure of the JSON data\n",
    "print_json_structure(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 3**:\n",
    "- This cell repeats the process from Cell 2 but with a different JSON file (`tweets_data4.json`). The function `print_json_structure` is used again to recursively print the keys and sub-keys, aiding in the inspection of the JSON data's structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "    List of 10 items\n",
      "        lang\n",
      "        edit_history_tweet_ids\n",
      "            List of 1 items\n",
      "        public_metrics\n",
      "            retweet_count\n",
      "            reply_count\n",
      "            like_count\n",
      "            quote_count\n",
      "            bookmark_count\n",
      "            impression_count\n",
      "        created_at\n",
      "        id\n",
      "        text\n",
      "        author_id\n",
      "includes\n",
      "    users\n",
      "        List of 10 items\n",
      "            location\n",
      "            username\n",
      "            id\n",
      "            verified\n",
      "            name\n",
      "meta\n",
      "    newest_id\n",
      "    oldest_id\n",
      "    result_count\n",
      "    next_token\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def print_json_structure(data, indent=''):\n",
    "    \"\"\"\n",
    "    Recursively prints the keys and sub-keys of a JSON-like dictionary or list.\n",
    "    Args:\n",
    "        data (dict or list): The JSON-like dictionary or list.\n",
    "        indent (str): The indentation string to visualize hierarchy.\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            print(f\"{indent}{key}\")\n",
    "            # Recursively print the structure of the nested dictionary or list\n",
    "            print_json_structure(value, indent + '    ')\n",
    "    elif isinstance(data, list):\n",
    "        print(f\"{indent}List of {len(data)} items\")\n",
    "        # Optionally, you can inspect the first item more closely if all items are expected to be similar\n",
    "        if data:\n",
    "            print_json_structure(data[0], indent + '    ')\n",
    "\n",
    "\n",
    "# Load your JSON data\n",
    "file_path = 'tweets_data4.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Print the structure of the JSON data\n",
    "print_json_structure(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 4**:\n",
    "- This cell defines two functions:\n",
    "  - `read_and_combine_json_files`: Reads multiple JSON files, combines their content into a single list of dictionaries, ensures each tweet has a unique identifier by renaming and handling the `id` key.\n",
    "  - `export_to_csv`: Converts the combined list of dictionaries to a pandas DataFrame and saves it as a CSV file.\n",
    "- It specifies paths to multiple JSON files, combines the data from these files using the defined functions, and exports the combined data to a CSV file (`combined_tweets_data.csv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2440 unique tweets to 'combined_tweets_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_and_combine_json_files(file_paths):\n",
    "    all_tweets = {}\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                data = json.load(file)\n",
    "                # Adjust to handle 'data' key or direct list based on JSON structure\n",
    "                tweets = data.get('data', data) if isinstance(\n",
    "                    data, dict) else data\n",
    "\n",
    "                for tweet in tweets:\n",
    "                    # Rename 'id' to 'tweet_id' and ensure uniqueness\n",
    "                    tweet_id = tweet.get('id')\n",
    "                    if tweet_id and tweet_id not in all_tweets:\n",
    "                        tweet['tweet_id'] = tweet_id\n",
    "                        del tweet['id']  # Remove the old 'id' key\n",
    "                        all_tweets[tweet_id] = tweet\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON from {file_path}: {e}\")\n",
    "        except KeyError as e:\n",
    "            print(f\"Missing expected key in {file_path}: {e}\")\n",
    "\n",
    "    return list(all_tweets.values())\n",
    "\n",
    "\n",
    "def export_to_csv(tweets, output_file):\n",
    "    # Convert list of dictionaries to DataFrame\n",
    "    df = pd.DataFrame(tweets)\n",
    "    # Save DataFrame to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved {len(df)} unique tweets to '{output_file}'.\")\n",
    "\n",
    "\n",
    "# Define the paths to your JSON files\n",
    "file_paths = [\n",
    "    'tweets_data.json', 'tweets_data1.json', 'tweets_data2.json',\n",
    "    'tweets_data4.json', 'tweets_data5.json', 'tweets_data6.json',\n",
    "    'tweets_data7.json', 'tweets_data8.json'\n",
    "]\n",
    "\n",
    "# Read and combine the data from the JSON files\n",
    "combined_tweets = read_and_combine_json_files(file_paths)\n",
    "\n",
    "# Export the combined data to a CSV file\n",
    "export_to_csv(combined_tweets, 'combined_tweets_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 5**:\n",
    "- This cell performs exploratory data analysis (EDA) on the combined dataset from `combined_tweets_data.csv`:\n",
    "  - Loads the dataset into a pandas DataFrame.\n",
    "  - Displays basic information about the dataset, including the number of entries and column types.\n",
    "  - Shows the data types of each column.\n",
    "  - Displays the number of non-null entries for each column.\n",
    "  - Displays the number of missing entries for each column.\n",
    "  - Shows the number of unique values for each column.\n",
    "  - Provides a preview of the first few rows of the dataset.\n",
    "  - Provides a basic statistical summary of numeric columns.\n",
    "- This analysis helps understand the structure, completeness, and statistical properties of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2440 entries, 0 to 2439\n",
      "Data columns (total 12 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   public_metrics          2440 non-null   object \n",
      " 1   text                    2440 non-null   object \n",
      " 2   conversation_id         2388 non-null   float64\n",
      " 3   edit_history_tweet_ids  2440 non-null   object \n",
      " 4   lang                    2440 non-null   object \n",
      " 5   referenced_tweets       1450 non-null   object \n",
      " 6   author_id               2390 non-null   float64\n",
      " 7   context_annotations     977 non-null    object \n",
      " 8   created_at              2440 non-null   object \n",
      " 9   tweet_id                2440 non-null   int64  \n",
      " 10  in_reply_to_user_id     439 non-null    float64\n",
      " 11  geo                     9 non-null      object \n",
      "dtypes: float64(3), int64(1), object(8)\n",
      "memory usage: 228.9+ KB\n",
      "None\n",
      "\n",
      "Data Types:\n",
      "public_metrics             object\n",
      "text                       object\n",
      "conversation_id           float64\n",
      "edit_history_tweet_ids     object\n",
      "lang                       object\n",
      "referenced_tweets          object\n",
      "author_id                 float64\n",
      "context_annotations        object\n",
      "created_at                 object\n",
      "tweet_id                    int64\n",
      "in_reply_to_user_id       float64\n",
      "geo                        object\n",
      "dtype: object\n",
      "\n",
      "Non-Null Count:\n",
      "public_metrics            2440\n",
      "text                      2440\n",
      "conversation_id           2388\n",
      "edit_history_tweet_ids    2440\n",
      "lang                      2440\n",
      "referenced_tweets         1450\n",
      "author_id                 2390\n",
      "context_annotations        977\n",
      "created_at                2440\n",
      "tweet_id                  2440\n",
      "in_reply_to_user_id        439\n",
      "geo                          9\n",
      "dtype: int64\n",
      "\n",
      "Missing Values Count:\n",
      "public_metrics               0\n",
      "text                         0\n",
      "conversation_id             52\n",
      "edit_history_tweet_ids       0\n",
      "lang                         0\n",
      "referenced_tweets          990\n",
      "author_id                   50\n",
      "context_annotations       1463\n",
      "created_at                   0\n",
      "tweet_id                     0\n",
      "in_reply_to_user_id       2001\n",
      "geo                       2431\n",
      "dtype: int64\n",
      "\n",
      "Unique Values Count:\n",
      "public_metrics             724\n",
      "text                      1717\n",
      "conversation_id           2255\n",
      "edit_history_tweet_ids    2437\n",
      "lang                        17\n",
      "referenced_tweets          668\n",
      "author_id                 1951\n",
      "context_annotations        350\n",
      "created_at                2421\n",
      "tweet_id                  2440\n",
      "in_reply_to_user_id        317\n",
      "geo                          9\n",
      "dtype: int64\n",
      "\n",
      "Preview of Data:\n",
      "                                      public_metrics  \\\n",
      "0  {'retweet_count': 6, 'reply_count': 0, 'like_c...   \n",
      "1  {'retweet_count': 0, 'reply_count': 0, 'like_c...   \n",
      "2  {'retweet_count': 0, 'reply_count': 0, 'like_c...   \n",
      "3  {'retweet_count': 4, 'reply_count': 0, 'like_c...   \n",
      "4  {'retweet_count': 0, 'reply_count': 0, 'like_c...   \n",
      "\n",
      "                                                text  conversation_id  \\\n",
      "0  RT @RestartProject: It appears the government ...     1.767657e+18   \n",
      "1  The U.S. Department of Energy's new Electronic...     1.767650e+18   \n",
      "2  Keep UCF green with 4Green Planet's electronic...     1.767649e+18   \n",
      "3  RT @TheDrum: Dell tackles e-waste with giant 3...     1.767647e+18   \n",
      "4  @davidfickling @IEA Rich countries import moun...     1.767438e+18   \n",
      "\n",
      "    edit_history_tweet_ids lang  \\\n",
      "0  ['1767656760607719717']   en   \n",
      "1  ['1767649960193913093']   en   \n",
      "2  ['1767649125540008328']   en   \n",
      "3  ['1767646850444648640']   en   \n",
      "4  ['1767645630338584718']   en   \n",
      "\n",
      "                                   referenced_tweets     author_id  \\\n",
      "0  [{'type': 'retweeted', 'id': '1767483408089833...  1.333596e+09   \n",
      "1                                                NaN  2.260200e+08   \n",
      "2                                                NaN  1.715111e+18   \n",
      "3  [{'type': 'retweeted', 'id': '1767278213754188...  7.917132e+07   \n",
      "4  [{'type': 'replied_to', 'id': '176743802119935...  1.062253e+18   \n",
      "\n",
      "                                 context_annotations  \\\n",
      "0  [{'domain': {'id': '10', 'name': 'Person', 'de...   \n",
      "1  [{'domain': {'id': '45', 'name': 'Brand Vertic...   \n",
      "2                                                NaN   \n",
      "3  [{'domain': {'id': '29', 'name': 'Events [Enti...   \n",
      "4  [{'domain': {'id': '30', 'name': 'Entities [En...   \n",
      "\n",
      "                 created_at             tweet_id  in_reply_to_user_id  geo  \n",
      "0  2024-03-12T20:59:47.000Z  1767656760607719717                  NaN  NaN  \n",
      "1  2024-03-12T20:32:46.000Z  1767649960193913093                  NaN  NaN  \n",
      "2  2024-03-12T20:29:27.000Z  1767649125540008328                  NaN  NaN  \n",
      "3  2024-03-12T20:20:25.000Z  1767646850444648640                  NaN  NaN  \n",
      "4  2024-03-12T20:15:34.000Z  1767645630338584718          233913342.0  NaN  \n",
      "\n",
      "Statistical Summary of Numeric Columns:\n",
      "       conversation_id     author_id      tweet_id  in_reply_to_user_id\n",
      "count     2.388000e+03  2.390000e+03  2.440000e+03         4.390000e+02\n",
      "mean      1.762755e+18  7.136270e+17  1.762799e+18         6.279488e+17\n",
      "std       4.840185e+15  7.057288e+17  3.995114e+15         6.789884e+17\n",
      "min       1.634546e+18  6.124730e+05  1.757409e+18         6.124730e+05\n",
      "25%       1.758673e+18  4.313188e+08  1.758643e+18         1.754350e+08\n",
      "50%       1.765242e+18  7.779150e+17  1.765248e+18         3.865683e+09\n",
      "75%       1.766668e+18  1.407295e+18  1.766693e+18         1.292323e+18\n",
      "max       1.767657e+18  1.766905e+18  1.767657e+18         1.765109e+18\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('combined_tweets_data.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Basic Information:\")\n",
    "print(df.info())\n",
    "\n",
    "# Show data types of each column\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Display the number of non-null entries for each column\n",
    "print(\"\\nNon-Null Count:\")\n",
    "print(df.notnull().sum())\n",
    "\n",
    "# Display the number of missing entries for each column\n",
    "print(\"\\nMissing Values Count:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Show the number of unique values for each column\n",
    "print(\"\\nUnique Values Count:\")\n",
    "print(df.nunique())\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "print(\"\\nPreview of Data:\")\n",
    "print(df.head())\n",
    "\n",
    "# Basic statistical summary of numeric columns\n",
    "print(\"\\nStatistical Summary of Numeric Columns:\")\n",
    "print(df.describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
