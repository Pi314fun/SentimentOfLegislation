{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 1**:\n",
    "- This cell imports necessary libraries and modules (`pandas`, `numpy`, `torch`, `transformers`, `sklearn`, and `nltk`) and ensures NLTK resources are downloaded. It then loads a dataset of bills from a CSV file and displays data types, summary statistics for numerical columns, missing values, and the first few rows to understand the data format. The number of bills is also printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Truck\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Truck\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Truck\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Types:\n",
      "bill_id             int64\n",
      "bill_number        object\n",
      "title              object\n",
      "description        object\n",
      "url                object\n",
      "state_link         object\n",
      "status              int64\n",
      "status_date        object\n",
      "session_id          int64\n",
      "state_id            int64\n",
      "state              object\n",
      "body_id             int64\n",
      "current_body_id     int64\n",
      "sponsors           object\n",
      "subjects           object\n",
      "texts              object\n",
      "votes              object\n",
      "dtype: object\n",
      "\n",
      "\n",
      "Summary Statistics:\n",
      "            bill_id      status   session_id    state_id     body_id  \\\n",
      "count  4.440000e+02  444.000000   444.000000  444.000000  444.000000   \n",
      "mean   1.771129e+06    2.009009  2050.889640   26.396396   56.896396   \n",
      "std    6.339997e+04    1.523306    45.977853   15.238231   31.293817   \n",
      "min    1.636388e+06    1.000000  1986.000000    2.000000    1.000000   \n",
      "25%    1.714478e+06    1.000000  2016.000000   13.000000   31.000000   \n",
      "50%    1.783610e+06    1.000000  2034.000000   23.000000   54.500000   \n",
      "75%    1.821410e+06    2.250000  2111.000000   40.000000   80.000000   \n",
      "max    1.872604e+06    6.000000  2129.000000   52.000000  116.000000   \n",
      "\n",
      "       current_body_id  \n",
      "count       444.000000  \n",
      "mean         56.936937  \n",
      "std          31.288804  \n",
      "min           1.000000  \n",
      "25%          31.750000  \n",
      "50%          54.500000  \n",
      "75%          80.000000  \n",
      "max         116.000000  \n",
      "\n",
      "\n",
      "Missing Values:\n",
      "bill_id            0\n",
      "bill_number        0\n",
      "title              0\n",
      "description        0\n",
      "url                0\n",
      "state_link         0\n",
      "status             0\n",
      "status_date        0\n",
      "session_id         0\n",
      "state_id           0\n",
      "state              0\n",
      "body_id            0\n",
      "current_body_id    0\n",
      "sponsors           0\n",
      "subjects           0\n",
      "texts              0\n",
      "votes              0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Data Format (first few rows):\n",
      "   bill_id bill_number                                              title  \\\n",
      "0  1696211      HF1373  Consumer choice of fuel provided, rulemaking a...   \n",
      "1  1862762      HF4800  Original equipment manufacturer required to fa...   \n",
      "2  1862691      HF4790  State Board of Investment standards to require...   \n",
      "3  1642978        HF30  Catalytic converter purchase or acquisition re...   \n",
      "4  1856477      HF4331  Metropolitan Council abolished, duties transfe...   \n",
      "\n",
      "                                         description  \\\n",
      "0  Consumer choice of fuel provided, rulemaking a...   \n",
      "1  Original equipment manufacturer required to fa...   \n",
      "2  State Board of Investment standards to require...   \n",
      "3  Catalytic converter purchase or acquisition re...   \n",
      "4  Metropolitan Council abolished, duties transfe...   \n",
      "\n",
      "                                        url  \\\n",
      "0  https://legiscan.com/MN/bill/HF1373/2023   \n",
      "1  https://legiscan.com/MN/bill/HF4800/2023   \n",
      "2  https://legiscan.com/MN/bill/HF4790/2023   \n",
      "3    https://legiscan.com/MN/bill/HF30/2023   \n",
      "4  https://legiscan.com/MN/bill/HF4331/2023   \n",
      "\n",
      "                                          state_link  status status_date  \\\n",
      "0  https://www.revisor.mn.gov/bills/bill.php?b=Ho...       1  2023-02-06   \n",
      "1  https://www.revisor.mn.gov/bills/bill.php?b=Ho...       1  2024-03-11   \n",
      "2  https://www.revisor.mn.gov/bills/bill.php?b=Ho...       1  2024-03-11   \n",
      "3  https://www.revisor.mn.gov/bills/bill.php?b=Ho...       4  2023-03-16   \n",
      "4  https://www.revisor.mn.gov/bills/bill.php?b=Ho...       1  2024-02-28   \n",
      "\n",
      "   session_id  state_id state  body_id  current_body_id  \\\n",
      "0        1986        23    MN       55               55   \n",
      "1        1986        23    MN       55               55   \n",
      "2        1986        23    MN       55               55   \n",
      "3        1986        23    MN       55               55   \n",
      "4        1986        23    MN       55               55   \n",
      "\n",
      "                                            sponsors subjects  \\\n",
      "0  [{'people_id': 10502, 'person_hash': 'fhkyu0ol...       []   \n",
      "1  [{'people_id': 23672, 'person_hash': 'grgnrgsp...       []   \n",
      "2  [{'people_id': 23655, 'person_hash': 'wfskfwfs...       []   \n",
      "3  [{'people_id': 20458, 'person_hash': '6yy7mlgi...       []   \n",
      "4  [{'people_id': 23680, 'person_hash': '5hg1lnqf...       []   \n",
      "\n",
      "                                               texts  \\\n",
      "0  [{'doc_id': 2683228, 'date': '2023-02-06', 'ty...   \n",
      "1  [{'doc_id': 2956902, 'date': '2024-03-08', 'ty...   \n",
      "2  [{'doc_id': 2956837, 'date': '2024-03-08', 'ty...   \n",
      "3  [{'doc_id': 2623214, 'date': '2023-01-04', 'ty...   \n",
      "4  [{'doc_id': 2944327, 'date': '2024-02-27', 'ty...   \n",
      "\n",
      "                                               votes  \n",
      "0                                                 []  \n",
      "1                                                 []  \n",
      "2                                                 []  \n",
      "3  [{'roll_call_id': 1257025, 'date': '2023-02-20...  \n",
      "4                                                 []  \n",
      "Number of bills: 444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the dataset\n",
    "bills_df = pd.read_csv('BillData/refined_detailed_bills.csv')\n",
    "\n",
    "# Display data types\n",
    "print(\"Data Types:\")\n",
    "print(bills_df.dtypes)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Display summary statistics for numerical columns\n",
    "print(\"Summary Statistics:\")\n",
    "print(bills_df.describe())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Identify missing values\n",
    "print(\"Missing Values:\")\n",
    "missing_values = bills_df.isnull().sum()\n",
    "print(missing_values)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Display the first few rows of the dataframe to understand the data format\n",
    "print(\"Data Format (first few rows):\")\n",
    "print(bills_df.head())\n",
    "\n",
    "print(f\"Number of bills: {bills_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 2**:\n",
    "- This cell defines two functions:\n",
    "  - `safe_json_loads`: Safely loads JSON data by correcting common format mistakes.\n",
    "  - `clean_text`: Cleans text by converting it to lowercase and removing non-word characters.\n",
    "  - `preprocess_text`: Tokenizes, removes stopwords, and lemmatizes the input text.\n",
    "- It then loads the bill data, drops unnecessary columns, converts date columns to datetime format, combines title and description into a single text column, cleans and preprocesses the text, creates a state mapping, and saves the processed data to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to load data...\n",
      "Data loaded successfully.\n",
      "Converting date columns...\n",
      "Conversion completed.\n",
      "Combining title and description...\n",
      "Text combined and cleaned.\n",
      "Applying text preprocessing...\n",
      "Text preprocessing completed.\n",
      "State map saved to 'BillData/state_map.json'.\n",
      "Saving processed data...\n",
      "Data saved successfully to 'BillData/final_processed_bills.csv'.\n"
     ]
    }
   ],
   "source": [
    "def safe_json_loads(s):\n",
    "    \"\"\"\n",
    "    Safely loads JSON data correcting common format mistakes.\n",
    "    \n",
    "    Args:\n",
    "    s (str): A string representation of JSON data.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary loaded from the JSON, empty if an error occurs.\n",
    "    \"\"\"\n",
    "    s = re.sub(r\"([{|,]\\s*'?)(\\w+)'?\\s*:\", r'\\1\"\\2\":', s)  # Fix keys\n",
    "    s = re.sub(r\":\\s*'([^']+)'(\\s*[},])\", r': \"\\1\"\\2', s)  # Fix values\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans text by converting to lower case and removing non-word characters.\n",
    "    \n",
    "    Args:\n",
    "    text (pd.Series): Pandas Series containing text data.\n",
    "    \n",
    "    Returns:\n",
    "    pd.Series: Cleaned text data.\n",
    "    \"\"\"\n",
    "    return text.str.lower().str.replace(r'\\W', ' ', regex=True)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Tokenizes, removes stopwords, and lemmatizes the input text.\n",
    "    \n",
    "    Args:\n",
    "    text (str): Text to preprocess.\n",
    "    \n",
    "    Returns:\n",
    "    str: Preprocessed text.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [\n",
    "        word for word in tokens if word not in stopwords.words('english')]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in tokens])\n",
    "\n",
    "\n",
    "print(\"Starting to load data...\")\n",
    "bills_df = pd.read_csv('BillData/refined_detailed_bills.csv')\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "# Drop unnecessary columns\n",
    "bills_df.drop(['state_link', 'sponsors', 'votes', 'url'], axis=1, inplace=True)\n",
    "\n",
    "print(\"Converting date columns...\")\n",
    "bills_df['status_date'] = pd.to_datetime(bills_df['status_date'])\n",
    "print(\"Conversion completed.\")\n",
    "\n",
    "print(\"Combining title and description...\")\n",
    "bills_df['full_text'] = bills_df['title'] + ' ' + bills_df['description']\n",
    "bills_df['full_text'] = clean_text(bills_df['full_text'])\n",
    "print(\"Text combined and cleaned.\")\n",
    "\n",
    "print(\"Applying text preprocessing...\")\n",
    "bills_df['processed_text'] = bills_df['full_text'].apply(preprocess_text)\n",
    "print(\"Text preprocessing completed.\")\n",
    "\n",
    "# Create and save state mapping\n",
    "state_map = dict(enumerate(bills_df['state'].unique()))\n",
    "json.dump(state_map, open('BillData/state_map.json', 'w'))\n",
    "print(\"State map saved to 'BillData/state_map.json'.\")\n",
    "\n",
    "print(\"Saving processed data...\")\n",
    "bills_df.to_csv('BillData/final_processed_bills.csv', index=False)\n",
    "print(\"Data saved successfully to 'BillData/final_processed_bills.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 3**:\n",
    "- This cell defines a custom Dataset class for handling text data and functions to generate embeddings using a pre-trained RoBERTa model. It loads the preprocessed dataset, verifies columns, determines the device (CPU or GPU), loads the RoBERTa model and tokenizer, generates embeddings, and saves them to a .npy file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Using device: cpu\n",
      "Loading RoBERTa model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n",
      "Embeddings generation complete.\n",
      "Saving embeddings...\n",
      "Embeddings saved successfully to BillData/roberta_bills_embeddings.npy.\n",
      "Process completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "\n",
    "# Ensure Numpy prints arrays completely\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for handling text data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, texts):\n",
    "        \"\"\"\n",
    "        Initialize with a list of texts.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the length of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return the text at the given index.\n",
    "        \"\"\"\n",
    "        return self.texts[idx]\n",
    "\n",
    "\n",
    "def get_embeddings(model, tokenizer, texts, batch_size=16, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of texts using a pre-trained RoBERTa model.\n",
    "    \n",
    "    Args:\n",
    "    - model: Pre-trained RoBERTa model.\n",
    "    - tokenizer: Corresponding tokenizer.\n",
    "    - texts: List of texts to process.\n",
    "    - batch_size: Batch size for processing.\n",
    "    - device: Device to run the model on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Array of embeddings.\n",
    "    \"\"\"\n",
    "    dataset = TextDataset(texts)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    model = model.to(device)\n",
    "    all_embeddings = []\n",
    "\n",
    "    for batch_texts in data_loader:\n",
    "        try:\n",
    "            inputs = tokenizer(batch_texts, return_tensors=\"pt\",\n",
    "                               padding=True, truncation=True, max_length=512)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state.mean(\n",
    "                dim=1).detach().cpu().numpy()\n",
    "            all_embeddings.append(embeddings)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch: {e}\")\n",
    "\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "\n",
    "def save_embeddings(embeddings, file_name):\n",
    "    \"\"\"\n",
    "    Save embeddings to a .npy file for efficient loading and use in PyG.\n",
    "    \n",
    "    Args:\n",
    "    - embeddings: Embeddings to save.\n",
    "    - file_name: Name of the file to save the embeddings.\n",
    "    \"\"\"\n",
    "    np.save(file_name, embeddings)\n",
    "    print(f\"Embeddings saved successfully to {file_name}.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load preprocessed dataset\n",
    "        print(\"Loading dataset...\")\n",
    "        bills_df = pd.read_csv('BillData/final_processed_bills.csv')\n",
    "\n",
    "        # Verify columns\n",
    "        if 'processed_text' not in bills_df.columns:\n",
    "            raise ValueError(\"Processed text column not found in the dataset.\")\n",
    "\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "        # Load RoBERTa model and tokenizer\n",
    "        print(\"Loading RoBERTa model and tokenizer...\")\n",
    "        model = RobertaModel.from_pretrained('roberta-base')\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "        # Get the list of processed texts\n",
    "        texts = bills_df['processed_text'].tolist()\n",
    "\n",
    "        # Generate embeddings\n",
    "        print(\"Generating embeddings...\")\n",
    "        embeddings = get_embeddings(model, tokenizer, texts, device=device)\n",
    "        print(\"Embeddings generation complete.\")\n",
    "\n",
    "        # Save embeddings to .npy file\n",
    "        embeddings_file = \"BillData/roberta_bills_embeddings.npy\"\n",
    "        print(\"Saving embeddings...\")\n",
    "        save_embeddings(embeddings, embeddings_file)\n",
    "        print(\"Process completed successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 4**:\n",
    "- This cell loads the generated embeddings and the original dataset, ensures they have the same length, computes pairwise cosine similarity, identifies duplicate bills based on a similarity threshold, removes duplicates from both the DataFrame and embeddings array, and saves the cleaned data and embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 31 duplicate bills. Cleaned data and embeddings saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the embeddings and the original dataset\n",
    "embeddings = np.load(\"BillData/RoBERTa_bills_embeddings.npy\")\n",
    "bills_df = pd.read_csv('BillData/final_processed_bills.csv')\n",
    "\n",
    "# Ensure the embeddings and the DataFrame have the same length\n",
    "assert len(embeddings) == len(\n",
    "    bills_df), \"Mismatch between embeddings and DataFrame length.\"\n",
    "\n",
    "# Compute pairwise cosine similarity\n",
    "cosine_similarities = 1 - cdist(embeddings, embeddings, metric='cosine')\n",
    "\n",
    "# Define the similarity threshold for considering bills as duplicates\n",
    "similarity_threshold = 0.99999999\n",
    "\n",
    "\n",
    "def identify_duplicates(similarity_matrix, threshold):\n",
    "    \"\"\"\n",
    "    Identify duplicates in the similarity matrix based on the given threshold.\n",
    "    \n",
    "    Args:\n",
    "    - similarity_matrix: Pairwise cosine similarity matrix.\n",
    "    - threshold: Cosine similarity threshold to consider bills as duplicates.\n",
    "    \n",
    "    Returns:\n",
    "    - List of indices of duplicate bills to be removed.\n",
    "    \"\"\"\n",
    "    num_bills = similarity_matrix.shape[0]\n",
    "    duplicates = set()\n",
    "\n",
    "    for i in range(num_bills):\n",
    "        for j in range(i + 1, num_bills):\n",
    "            if similarity_matrix[i, j] > threshold:\n",
    "                duplicates.add(j)\n",
    "\n",
    "    return list(duplicates)\n",
    "\n",
    "\n",
    "# Identify duplicate bill indices\n",
    "duplicate_indices = identify_duplicates(\n",
    "    cosine_similarities, similarity_threshold)\n",
    "\n",
    "# Remove duplicates from the DataFrame\n",
    "bills_df_no_duplicates = bills_df.drop(\n",
    "    index=duplicate_indices).reset_index(drop=True)\n",
    "\n",
    "# Remove duplicates from the embeddings array\n",
    "embeddings_no_duplicates = np.delete(embeddings, duplicate_indices, axis=0)\n",
    "\n",
    "# Save the cleaned DataFrame\n",
    "bills_df_no_duplicates.to_csv(\n",
    "    'BillData/final_bills_no_duplicates.csv', index=False)\n",
    "\n",
    "# Save the updated embeddings array to a new file\n",
    "np.save(\"BillData/RoBERTa_bills_embeddings_no_duplicates.npy\",\n",
    "        embeddings_no_duplicates)\n",
    "\n",
    "print(f\"Removed {len(duplicate_indices)} duplicate bills. Cleaned data and embeddings saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 5**:\n",
    "- This cell applies dimensionality reduction using PCA to the embeddings and normalizes them using StandardScaler. It retains 95% variance in the PCA step, saves the updated embeddings, and updates the DataFrame with PCA embeddings, saving it to a CSV file.\n",
    "\n",
    "**PCA and normalization:**\n",
    "Apply dimensionality reduction using Principal Component Analysis (PCA), and normalization. This will help to spread out the embeddings in the feature space, reducing the likelihood of high similarity among embeddings and making connections more meaningful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated bill embeddings saved to 'BillData/roberta_bills_embeddings_pca.npy'.\n",
      "DataFrame with PCA embeddings saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the bill embeddings and the original dataset\n",
    "bill_embeddings = np.load(\n",
    "    \"BillData/RoBERTa_bills_embeddings_no_duplicates.npy\")\n",
    "bills_df = pd.read_csv('BillData/final_bills_no_duplicates.csv')\n",
    "\n",
    "# Ensure the embeddings and the DataFrame have the same length\n",
    "assert len(bill_embeddings) == len(\n",
    "    bills_df), \"Mismatch between bill embeddings and DataFrame length.\"\n",
    "\n",
    "# Standardize the embeddings\n",
    "scaler = StandardScaler()\n",
    "bill_embeddings_scaled = scaler.fit_transform(bill_embeddings)\n",
    "\n",
    "# Apply PCA to reduce dimensionality while retaining 95% variance\n",
    "pca = PCA(n_components=0.95)\n",
    "bill_embeddings_pca = pca.fit_transform(bill_embeddings_scaled)\n",
    "\n",
    "# Save the updated embeddings\n",
    "np.save(\"BillData/roberta_bills_embeddings_pca.npy\", bill_embeddings_pca)\n",
    "print(f\"Updated bill embeddings saved to 'BillData/roberta_bills_embeddings_pca.npy'.\")\n",
    "\n",
    "# Update the DataFrame with PCA embeddings\n",
    "bills_df['pca_embeddings'] = list(bill_embeddings_pca)\n",
    "bills_df.to_csv('BillData/bills.csv', index=False)\n",
    "\n",
    "print(\"DataFrame with PCA embeddings saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 6**:\n",
    "- This cell defines functions to load a sentiment analysis model and tokenizer, perform sentiment analysis on texts, and save the results. It loads the bills DataFrame, applies sentiment analysis to each bill's processed text, and saves the updated DataFrame and sentiment probabilities tensor to files. It also verifies the saved CSV file by loading and displaying the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment model and tokenizer loaded successfully.\n",
      "Loaded 413 bills from BillData/final_bills_no_duplicates.csv.\n",
      "Using device: cpu\n",
      "Sentiment analysis applied to all bills.\n",
      "Updated DataFrame saved to BillData/bills.csv.\n",
      "Tensor saved to BillData/bills_sentiments_tensor.pt.\n",
      "Verification successful. First few rows of BillData/bills.csv:\n",
      "   bill_id bill_number                                              title  \\\n",
      "0  1696211      HF1373  Consumer choice of fuel provided, rulemaking a...   \n",
      "1  1862762      HF4800  Original equipment manufacturer required to fa...   \n",
      "2  1862691      HF4790  State Board of Investment standards to require...   \n",
      "3  1642978        HF30  Catalytic converter purchase or acquisition re...   \n",
      "4  1856477      HF4331  Metropolitan Council abolished, duties transfe...   \n",
      "\n",
      "                                         description  status status_date  \\\n",
      "0  Consumer choice of fuel provided, rulemaking a...       1  2023-02-06   \n",
      "1  Original equipment manufacturer required to fa...       1  2024-03-11   \n",
      "2  State Board of Investment standards to require...       1  2024-03-11   \n",
      "3  Catalytic converter purchase or acquisition re...       4  2023-03-16   \n",
      "4  Metropolitan Council abolished, duties transfe...       1  2024-02-28   \n",
      "\n",
      "   session_id  state_id state  body_id  current_body_id subjects  \\\n",
      "0        1986        23    MN       55               55       []   \n",
      "1        1986        23    MN       55               55       []   \n",
      "2        1986        23    MN       55               55       []   \n",
      "3        1986        23    MN       55               55       []   \n",
      "4        1986        23    MN       55               55       []   \n",
      "\n",
      "                                               texts  \\\n",
      "0  [{'doc_id': 2683228, 'date': '2023-02-06', 'ty...   \n",
      "1  [{'doc_id': 2956902, 'date': '2024-03-08', 'ty...   \n",
      "2  [{'doc_id': 2956837, 'date': '2024-03-08', 'ty...   \n",
      "3  [{'doc_id': 2623214, 'date': '2023-01-04', 'ty...   \n",
      "4  [{'doc_id': 2944327, 'date': '2024-02-27', 'ty...   \n",
      "\n",
      "                                           full_text  \\\n",
      "0  consumer choice of fuel provided  rulemaking a...   \n",
      "1  original equipment manufacturer required to fa...   \n",
      "2  state board of investment standards to require...   \n",
      "3  catalytic converter purchase or acquisition re...   \n",
      "4  metropolitan council abolished  duties transfe...   \n",
      "\n",
      "                                      processed_text  bill_positive  \\\n",
      "0  consumer choice fuel provided rulemaking autho...       0.303996   \n",
      "1  original equipment manufacturer required facil...       0.192843   \n",
      "2  state board investment standard require sustai...       0.036222   \n",
      "3  catalytic converter purchase acquisition requi...       0.332358   \n",
      "4  metropolitan council abolished duty transferre...       0.281935   \n",
      "\n",
      "   bill_neutral  bill_negative  \n",
      "0      0.662853       0.033151  \n",
      "1      0.763062       0.044095  \n",
      "2      0.811128       0.152649  \n",
      "3      0.646202       0.021440  \n",
      "4      0.692732       0.025333  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_sentiment_model():\n",
    "    \"\"\"\n",
    "    Load the sentiment analysis model and tokenizer.\n",
    "    \n",
    "    Returns:\n",
    "    - tokenizer: Pre-trained tokenizer for sentiment analysis.\n",
    "    - model: Pre-trained sentiment analysis model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "        print(\"Sentiment model and tokenizer loaded successfully.\")\n",
    "        return tokenizer, model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sentiment model or tokenizer: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def sentiment_analysis(texts, tokenizer, model, batch_size=16, device='cpu'):\n",
    "    \"\"\"\n",
    "    Perform sentiment analysis on a list of texts using a pre-trained model.\n",
    "    \n",
    "    Args:\n",
    "    - texts: List of texts to analyze.\n",
    "    - tokenizer: Pre-trained tokenizer.\n",
    "    - model: Pre-trained sentiment analysis model.\n",
    "    - batch_size: Batch size for processing.\n",
    "    - device: Device to run the model on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Tensor containing sentiment probabilities for each text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = model.to(device)\n",
    "        all_scores = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            encoded_input = tokenizer(\n",
    "                batch_texts, return_tensors='pt', truncation=True, max_length=512, padding=True)\n",
    "            encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "            with torch.no_grad():\n",
    "                output = model(**encoded_input)\n",
    "            scores = torch.nn.functional.softmax(output.logits, dim=-1).cpu()\n",
    "            all_scores.append(scores)\n",
    "        return torch.cat(all_scores, dim=0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error performing sentiment analysis: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def load_bills(file_path):\n",
    "    \"\"\"\n",
    "    Load the bills and processed texts from a CSV file.\n",
    "    \n",
    "    Args:\n",
    "    - file_path: Path to the CSV file containing the bills and processed texts.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame containing the bills and their processed texts.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        bills_df = pd.read_csv(file_path)\n",
    "        print(f\"Loaded {len(bills_df)} bills from {file_path}.\")\n",
    "        return bills_df\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading file: {e}\")\n",
    "        raise\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"Error parsing file: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def apply_sentiment_analysis_to_bills(bills_df, tokenizer, model, batch_size=16, device='cpu'):\n",
    "    \"\"\"\n",
    "    Apply sentiment analysis to each bill's processed text in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "    - bills_df: DataFrame containing the bills.\n",
    "    - tokenizer: Pre-trained tokenizer.\n",
    "    - model: Pre-trained sentiment analysis model.\n",
    "    - batch_size: Batch size for processing.\n",
    "    - device: Device to run the model on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Updated DataFrame with sentiment probabilities.\n",
    "    - torch.Tensor: Tensor containing sentiment probabilities.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        texts = bills_df['processed_text'].tolist()\n",
    "        sentiments = sentiment_analysis(\n",
    "            texts, tokenizer, model, batch_size, device)\n",
    "        sentiments_df = pd.DataFrame(sentiments.numpy(), columns=[\n",
    "                                     'bill_positive', 'bill_neutral', 'bill_negative'])\n",
    "        bills_df = pd.concat([bills_df, sentiments_df], axis=1)\n",
    "        print(\"Sentiment analysis applied to all bills.\")\n",
    "        return bills_df, sentiments\n",
    "    except Exception as e:\n",
    "        print(f\"Error applying sentiment analysis: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def save_updated_bills_dataframe(bills_df, file_path):\n",
    "    \"\"\"\n",
    "    Save the updated DataFrame with sentiment probabilities to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "    - bills_df: DataFrame containing the updated bills.\n",
    "    - file_path: Path to save the updated CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        bills_df.to_csv(file_path, index=False)\n",
    "        print(f\"Updated DataFrame saved to {file_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving updated DataFrame: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def save_tensor(tensor, file_path):\n",
    "    \"\"\"\n",
    "    Save the tensor to a file.\n",
    "    \n",
    "    Args:\n",
    "    - tensor: Tensor to save.\n",
    "    - file_path: Path to save the tensor file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        torch.save(tensor, file_path)\n",
    "        print(f\"Tensor saved to {file_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving tensor: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def verify_saved_file(file_path):\n",
    "    \"\"\"\n",
    "    Verify the integrity of the saved CSV file by loading it and checking the first few rows.\n",
    "    \n",
    "    Args:\n",
    "    - file_path: Path to the saved CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Verification successful. First few rows of {file_path}:\")\n",
    "        print(df.head())\n",
    "    except Exception as e:\n",
    "        print(f\"Error verifying saved file: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load the sentiment analysis model and tokenizer\n",
    "        tokenizer, model = load_sentiment_model()\n",
    "\n",
    "        # Load the bills DataFrame\n",
    "        bills_df = load_bills('BillData/final_bills_no_duplicates.csv')\n",
    "\n",
    "        # Determine device\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "        # Apply sentiment analysis to the DataFrame\n",
    "        bills_df, sentiments_tensor = apply_sentiment_analysis_to_bills(\n",
    "            bills_df, tokenizer, model, device=device)\n",
    "\n",
    "        # Save the updated DataFrame\n",
    "        save_updated_bills_dataframe(bills_df, 'BillData/bills.csv')\n",
    "\n",
    "        # Save the tensor containing sentiment probabilities\n",
    "        save_tensor(sentiments_tensor, 'BillData/bills_sentiments_tensor.pt')\n",
    "\n",
    "        # Verify the saved file\n",
    "        verify_saved_file('BillData/bills.csv')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in the main execution block: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 7**:\n",
    "- This cell loads the processed bill embeddings, drops unnecessary columns, converts the 'status_date' column to datetime format, creates placeholders for sentiment columns, and saves the modified DataFrame. It outputs the DataFrame for troubleshooting purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to BillData/bills.csv\n",
      "   bill_id  status status_date  session_id  state_id  body_id  bill_positive  \\\n",
      "0  1696211       1  2023-02-06        1986        23       55       0.303996   \n",
      "1  1862762       1  2024-03-11        1986        23       55       0.192843   \n",
      "2  1862691       1  2024-03-11        1986        23       55       0.036222   \n",
      "3  1642978       4  2023-03-16        1986        23       55       0.332358   \n",
      "4  1856477       1  2024-02-28        1986        23       55       0.281935   \n",
      "\n",
      "   bill_neutral  bill_negative  positive  neutral  negative  \n",
      "0      0.662853       0.033151       0.0      0.0       0.0  \n",
      "1      0.763062       0.044095       0.0      0.0       0.0  \n",
      "2      0.811128       0.152649       0.0      0.0       0.0  \n",
      "3      0.646202       0.021440       0.0      0.0       0.0  \n",
      "4      0.692732       0.025333       0.0      0.0       0.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the processed bill embeddings\n",
    "bills_df = pd.read_csv('BillData/bills.csv')\n",
    "\n",
    "# List of columns to drop\n",
    "columns_to_drop = [\n",
    "    'bill_number', 'title', 'description', 'url', 'state_link', 'state',\n",
    "    'current_body_id', 'sponsors', 'subjects', 'texts', 'votes', 'processed_text', 'full_text'\n",
    "]\n",
    "\n",
    "# Check which columns are present in the DataFrame\n",
    "existing_columns_to_drop = [\n",
    "    col for col in columns_to_drop if col in bills_df.columns]\n",
    "\n",
    "# Drop the specified columns\n",
    "bills_df.drop(columns=existing_columns_to_drop, inplace=True)\n",
    "\n",
    "# Convert 'status_date' column to datetime format and focus on month, day, and year\n",
    "bills_df['status_date'] = pd.to_datetime(\n",
    "    bills_df['status_date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Create placeholders for positive, neutral, and negative sentiment columns\n",
    "bills_df['positive'] = 0.0\n",
    "bills_df['neutral'] = 0.0\n",
    "bills_df['negative'] = 0.0\n",
    "\n",
    "# Save the modified DataFrame\n",
    "output_path = 'BillData/bills.csv'\n",
    "bills_df.to_csv(output_path, index=False)\n",
    "\n",
    "# Output for troubleshooting\n",
    "print(f\"DataFrame saved to {output_path}\")\n",
    "print(bills_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 8**:\n",
    "- This cell converts the 'status_date' column in the DataFrame to Unix timestamp format, saves the updated DataFrame to a CSV file, and outputs the DataFrame and its data types for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to 'BillData/final_bills.csv'\n",
      "   bill_id  status  status_date  session_id  state_id  body_id  bill_positive  \\\n",
      "0  1696211       1   1675641600        1986        23       55       0.303996   \n",
      "1  1862762       1   1710115200        1986        23       55       0.192843   \n",
      "2  1862691       1   1710115200        1986        23       55       0.036222   \n",
      "3  1642978       4   1678924800        1986        23       55       0.332358   \n",
      "4  1856477       1   1709078400        1986        23       55       0.281935   \n",
      "\n",
      "   bill_neutral  bill_negative  positive  neutral  negative  \n",
      "0      0.662853       0.033151       0.0      0.0       0.0  \n",
      "1      0.763062       0.044095       0.0      0.0       0.0  \n",
      "2      0.811128       0.152649       0.0      0.0       0.0  \n",
      "3      0.646202       0.021440       0.0      0.0       0.0  \n",
      "4      0.692732       0.025333       0.0      0.0       0.0  \n",
      "bill_id            int64\n",
      "status             int64\n",
      "status_date        int64\n",
      "session_id         int64\n",
      "state_id           int64\n",
      "body_id            int64\n",
      "bill_positive    float64\n",
      "bill_neutral     float64\n",
      "bill_negative    float64\n",
      "positive         float64\n",
      "neutral          float64\n",
      "negative         float64\n",
      "dtype: object\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "bills_df = pd.read_csv('BillData/bills.csv')\n",
    "\n",
    "# Convert 'status_date' to datetime and then to int64\n",
    "bills_df['status_date'] = pd.to_datetime(bills_df['status_date'])\n",
    "bills_df['status_date'] = bills_df['status_date'].astype(\n",
    "    'int64') // 10**9  # Convert to Unix timestamp in seconds\n",
    "\n",
    "# Save the updated DataFrame\n",
    "bills_df.to_csv('BillData/final_bills.csv', index=False)\n",
    "\n",
    "# Output for verification\n",
    "print(f\"DataFrame saved to 'BillData/final_bills.csv'\")\n",
    "print(bills_df.head())\n",
    "print(bills_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 9**:\n",
    "- This cell displays data types, summary statistics for numerical columns, identifies missing values, and displays the first few rows of the DataFrame to understand the data format and completeness. It also prints the total number of bills in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Types:\n",
      "bill_id            int64\n",
      "status             int64\n",
      "status_date       object\n",
      "session_id         int64\n",
      "state_id           int64\n",
      "body_id            int64\n",
      "bill_positive    float64\n",
      "bill_neutral     float64\n",
      "bill_negative    float64\n",
      "positive         float64\n",
      "neutral          float64\n",
      "negative         float64\n",
      "dtype: object\n",
      "\n",
      "\n",
      "Summary Statistics:\n",
      "            bill_id      status   session_id    state_id     body_id  \\\n",
      "count  4.130000e+02  413.000000   413.000000  413.000000  413.000000   \n",
      "mean   1.770487e+06    2.041162  2049.861985   26.169492   56.786925   \n",
      "std    6.289655e+04    1.540946    45.679156   15.368621   31.502475   \n",
      "min    1.636388e+06    1.000000  1986.000000    2.000000    1.000000   \n",
      "25%    1.714481e+06    1.000000  2016.000000   13.000000   31.000000   \n",
      "50%    1.780652e+06    1.000000  2033.000000   23.000000   54.000000   \n",
      "75%    1.820955e+06    3.000000  2111.000000   40.000000   80.000000   \n",
      "max    1.872604e+06    6.000000  2129.000000   52.000000  116.000000   \n",
      "\n",
      "       bill_positive  bill_neutral  bill_negative  positive  neutral  negative  \n",
      "count     413.000000    413.000000     413.000000     413.0    413.0     413.0  \n",
      "mean        0.127151      0.787452       0.085397       0.0      0.0       0.0  \n",
      "std         0.118393      0.122444       0.107835       0.0      0.0       0.0  \n",
      "min         0.002278      0.271643       0.007211       0.0      0.0       0.0  \n",
      "25%         0.048044      0.747709       0.029581       0.0      0.0       0.0  \n",
      "50%         0.095754      0.818461       0.043957       0.0      0.0       0.0  \n",
      "75%         0.165196      0.869456       0.080961       0.0      0.0       0.0  \n",
      "max         0.719482      0.940168       0.706097       0.0      0.0       0.0  \n",
      "\n",
      "\n",
      "Missing Values:\n",
      "bill_id          0\n",
      "status           0\n",
      "status_date      0\n",
      "session_id       0\n",
      "state_id         0\n",
      "body_id          0\n",
      "bill_positive    0\n",
      "bill_neutral     0\n",
      "bill_negative    0\n",
      "positive         0\n",
      "neutral          0\n",
      "negative         0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Data Format (first few rows):\n",
      "   bill_id  status status_date  session_id  state_id  body_id  bill_positive  \\\n",
      "0  1696211       1  2023-02-06        1986        23       55       0.303996   \n",
      "1  1862762       1  2024-03-11        1986        23       55       0.192843   \n",
      "2  1862691       1  2024-03-11        1986        23       55       0.036222   \n",
      "3  1642978       4  2023-03-16        1986        23       55       0.332358   \n",
      "4  1856477       1  2024-02-28        1986        23       55       0.281935   \n",
      "\n",
      "   bill_neutral  bill_negative  positive  neutral  negative  \n",
      "0      0.662853       0.033151       0.0      0.0       0.0  \n",
      "1      0.763062       0.044095       0.0      0.0       0.0  \n",
      "2      0.811128       0.152649       0.0      0.0       0.0  \n",
      "3      0.646202       0.021440       0.0      0.0       0.0  \n",
      "4      0.692732       0.025333       0.0      0.0       0.0  \n",
      "Number of bills: 413\n"
     ]
    }
   ],
   "source": [
    "# Display data types\n",
    "print(\"Data Types:\")\n",
    "print(bills_df.dtypes)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Display summary statistics for numerical columns\n",
    "print(\"Summary Statistics:\")\n",
    "print(bills_df.describe())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Identify missing values\n",
    "print(\"Missing Values:\")\n",
    "missing_values = bills_df.isnull().sum()\n",
    "print(missing_values)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Display the first few rows of the dataframe to understand the data format\n",
    "print(\"Data Format (first few rows):\")\n",
    "print(bills_df.head())\n",
    "\n",
    "print(f\"Number of bills: {bills_df.shape[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
