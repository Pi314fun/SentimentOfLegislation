{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 1**:\n",
    "- **Data Loading and Initial Exploration**: This cell loads the tweet dataset, displays data types, summary statistics for numerical columns, identifies missing values, and displays the first few rows of the DataFrame to understand its format. It also prints the number of tweets in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Types:\n",
      "public_metrics             object\n",
      "text                       object\n",
      "conversation_id           float64\n",
      "edit_history_tweet_ids     object\n",
      "lang                       object\n",
      "referenced_tweets          object\n",
      "author_id                 float64\n",
      "context_annotations        object\n",
      "created_at                 object\n",
      "tweet_id                    int64\n",
      "in_reply_to_user_id       float64\n",
      "geo                        object\n",
      "metrics                    object\n",
      "total_engagement            int64\n",
      "log_engage                float64\n",
      "dtype: object\n",
      "\n",
      "\n",
      "Summary Statistics:\n",
      "       conversation_id     author_id      tweet_id  in_reply_to_user_id  \\\n",
      "count     2.388000e+03  2.390000e+03  2.440000e+03         4.390000e+02   \n",
      "mean      1.762755e+18  7.136270e+17  1.762799e+18         6.279488e+17   \n",
      "std       4.840185e+15  7.057288e+17  3.995114e+15         6.789884e+17   \n",
      "min       1.634546e+18  6.124730e+05  1.757409e+18         6.124730e+05   \n",
      "25%       1.758673e+18  4.313188e+08  1.758643e+18         1.754350e+08   \n",
      "50%       1.765242e+18  7.779150e+17  1.765248e+18         3.865683e+09   \n",
      "75%       1.766668e+18  1.407295e+18  1.766693e+18         1.292323e+18   \n",
      "max       1.767657e+18  1.766905e+18  1.767657e+18         1.765109e+18   \n",
      "\n",
      "       total_engagement   log_engage  \n",
      "count       2440.000000  2440.000000  \n",
      "mean         630.920082     3.088148  \n",
      "std        17344.489663     1.735799  \n",
      "min            0.000000     0.000000  \n",
      "25%            5.000000     1.791759  \n",
      "50%           19.000000     2.995732  \n",
      "75%           59.250000     4.098477  \n",
      "max       844519.000000    13.646524  \n",
      "\n",
      "\n",
      "Missing Values:\n",
      "public_metrics               0\n",
      "text                         0\n",
      "conversation_id             52\n",
      "edit_history_tweet_ids       0\n",
      "lang                         0\n",
      "referenced_tweets          990\n",
      "author_id                   50\n",
      "context_annotations       1463\n",
      "created_at                   0\n",
      "tweet_id                     0\n",
      "in_reply_to_user_id       2001\n",
      "geo                       2431\n",
      "metrics                      0\n",
      "total_engagement             0\n",
      "log_engage                   0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Data Format (first few rows):\n",
      "                                      public_metrics  \\\n",
      "0  {'retweet_count': 6, 'reply_count': 0, 'like_c...   \n",
      "1  {'retweet_count': 0, 'reply_count': 0, 'like_c...   \n",
      "2  {'retweet_count': 0, 'reply_count': 0, 'like_c...   \n",
      "3  {'retweet_count': 4, 'reply_count': 0, 'like_c...   \n",
      "4  {'retweet_count': 0, 'reply_count': 0, 'like_c...   \n",
      "\n",
      "                                                text  conversation_id  \\\n",
      "0  RT @RestartProject: It appears the government ...     1.767657e+18   \n",
      "1  The U.S. Department of Energy's newÂ Electronic...     1.767650e+18   \n",
      "2  Keep UCF green with 4Green Planet's electronic...     1.767649e+18   \n",
      "3  RT @TheDrum: Dell tackles e-waste with giant 3...     1.767647e+18   \n",
      "4  @davidfickling @IEA Rich countries import moun...     1.767438e+18   \n",
      "\n",
      "    edit_history_tweet_ids lang  \\\n",
      "0  ['1767656760607719717']   en   \n",
      "1  ['1767649960193913093']   en   \n",
      "2  ['1767649125540008328']   en   \n",
      "3  ['1767646850444648640']   en   \n",
      "4  ['1767645630338584718']   en   \n",
      "\n",
      "                                   referenced_tweets     author_id  \\\n",
      "0  [{'type': 'retweeted', 'id': '1767483408089833...  1.333596e+09   \n",
      "1                                                NaN  2.260200e+08   \n",
      "2                                                NaN  1.715111e+18   \n",
      "3  [{'type': 'retweeted', 'id': '1767278213754188...  7.917132e+07   \n",
      "4  [{'type': 'replied_to', 'id': '176743802119935...  1.062253e+18   \n",
      "\n",
      "                                 context_annotations  \\\n",
      "0  [{'domain': {'id': '10', 'name': 'Person', 'de...   \n",
      "1  [{'domain': {'id': '45', 'name': 'Brand Vertic...   \n",
      "2                                                NaN   \n",
      "3  [{'domain': {'id': '29', 'name': 'Events [Enti...   \n",
      "4  [{'domain': {'id': '30', 'name': 'Entities [En...   \n",
      "\n",
      "                 created_at             tweet_id  in_reply_to_user_id  geo  \\\n",
      "0  2024-03-12T20:59:47.000Z  1767656760607719717                  NaN  NaN   \n",
      "1  2024-03-12T20:32:46.000Z  1767649960193913093                  NaN  NaN   \n",
      "2  2024-03-12T20:29:27.000Z  1767649125540008328                  NaN  NaN   \n",
      "3  2024-03-12T20:20:25.000Z  1767646850444648640                  NaN  NaN   \n",
      "4  2024-03-12T20:15:34.000Z  1767645630338584718          233913342.0  NaN   \n",
      "\n",
      "                                             metrics  total_engagement  \\\n",
      "0  {'retweet_count': 6, 'reply_count': 0, 'like_c...                 6   \n",
      "1  {'retweet_count': 0, 'reply_count': 0, 'like_c...                50   \n",
      "2  {'retweet_count': 0, 'reply_count': 0, 'like_c...                 1   \n",
      "3  {'retweet_count': 4, 'reply_count': 0, 'like_c...                 4   \n",
      "4  {'retweet_count': 0, 'reply_count': 0, 'like_c...                 4   \n",
      "\n",
      "   log_engage  \n",
      "0    1.945910  \n",
      "1    3.931826  \n",
      "2    0.693147  \n",
      "3    1.609438  \n",
      "4    1.609438  \n",
      "Number of tweets: 2440\n"
     ]
    }
   ],
   "source": [
    "# 1. Data Loading and Initial Exploration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# Load the dataset\n",
    "tweets_df = pd.read_csv('TweetData/combined_tweets_data.csv')\n",
    "\n",
    "# Display data types\n",
    "print(\"Data Types:\")\n",
    "print(tweets_df.dtypes)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Display summary statistics for numerical columns\n",
    "print(\"Summary Statistics:\")\n",
    "print(tweets_df.describe())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Identify missing values\n",
    "print(\"Missing Values:\")\n",
    "missing_values = tweets_df.isnull().sum()\n",
    "print(missing_values)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Display the first few rows of the dataframe to understand the data format\n",
    "print(\"Data Format (first few rows):\")\n",
    "print(tweets_df.head())\n",
    "\n",
    "print(f\"Number of tweets: {tweets_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 2**:\n",
    "- **Text Preprocessing**: This cell imports necessary NLP libraries and defines a function to preprocess text by converting it to lowercase, removing URLs, mentions, hashtags, punctuation, and applying tokenization and lemmatization. It downloads necessary NLTK resources and applies the preprocessing function to the tweet text column, saving the processed DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Truck\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Truck\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Truck\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                          public_metrics  \\\n",
      "0     {'retweet_count': 6, 'reply_count': 0, 'like_c...   \n",
      "1     {'retweet_count': 0, 'reply_count': 0, 'like_c...   \n",
      "2     {'retweet_count': 0, 'reply_count': 0, 'like_c...   \n",
      "3     {'retweet_count': 4, 'reply_count': 0, 'like_c...   \n",
      "4     {'retweet_count': 0, 'reply_count': 0, 'like_c...   \n",
      "...                                                 ...   \n",
      "2435  {'retweet_count': 0, 'reply_count': 0, 'like_c...   \n",
      "2436  {'retweet_count': 0, 'reply_count': 0, 'like_c...   \n",
      "2437  {'retweet_count': 2, 'reply_count': 0, 'like_c...   \n",
      "2438  {'retweet_count': 2, 'reply_count': 0, 'like_c...   \n",
      "2439  {'retweet_count': 0, 'reply_count': 0, 'like_c...   \n",
      "\n",
      "                                                   text  conversation_id  \\\n",
      "0     RT @RestartProject: It appears the government ...     1.767657e+18   \n",
      "1     The U.S. Department of Energy's newÂ Electronic...     1.767650e+18   \n",
      "2     Keep UCF green with 4Green Planet's electronic...     1.767649e+18   \n",
      "3     RT @TheDrum: Dell tackles e-waste with giant 3...     1.767647e+18   \n",
      "4     @davidfickling @IEA Rich countries import moun...     1.767438e+18   \n",
      "...                                                 ...              ...   \n",
      "2435  Consumers buy products to get specific jobs do...              NaN   \n",
      "2436  Don't miss out on ep. 2â£ of our 'Wicked Sustai...              NaN   \n",
      "2437  RT @NationalLib_GoI: An exhibition was organiz...              NaN   \n",
      "2438  RT @NationalLib_GoI: An exhibition was organiz...              NaN   \n",
      "2439  Discover how Wisetek is leading the charge in ...     1.759873e+18   \n",
      "\n",
      "       edit_history_tweet_ids lang  \\\n",
      "0     ['1767656760607719717']   en   \n",
      "1     ['1767649960193913093']   en   \n",
      "2     ['1767649125540008328']   en   \n",
      "3     ['1767646850444648640']   en   \n",
      "4     ['1767645630338584718']   en   \n",
      "...                       ...  ...   \n",
      "2435  ['1757409041905562081']   en   \n",
      "2436  ['1757409037530919300']   en   \n",
      "2437  ['1759467758570619388']   en   \n",
      "2438  ['1758002237065097265']   en   \n",
      "2439  ['1759872858484851152']   en   \n",
      "\n",
      "                                      referenced_tweets     author_id  \\\n",
      "0     [{'type': 'retweeted', 'id': '1767483408089833...  1.333596e+09   \n",
      "1                                                   NaN  2.260200e+08   \n",
      "2                                                   NaN  1.715111e+18   \n",
      "3     [{'type': 'retweeted', 'id': '1767278213754188...  7.917132e+07   \n",
      "4     [{'type': 'replied_to', 'id': '176743802119935...  1.062253e+18   \n",
      "...                                                 ...           ...   \n",
      "2435                                                NaN           NaN   \n",
      "2436                                                NaN           NaN   \n",
      "2437                                                NaN  1.469714e+18   \n",
      "2438                                                NaN  1.469714e+18   \n",
      "2439                                                NaN  2.973132e+08   \n",
      "\n",
      "                                    context_annotations  \\\n",
      "0     [{'domain': {'id': '10', 'name': 'Person', 'de...   \n",
      "1     [{'domain': {'id': '45', 'name': 'Brand Vertic...   \n",
      "2                                                   NaN   \n",
      "3     [{'domain': {'id': '29', 'name': 'Events [Enti...   \n",
      "4     [{'domain': {'id': '30', 'name': 'Entities [En...   \n",
      "...                                                 ...   \n",
      "2435                                                NaN   \n",
      "2436                                                NaN   \n",
      "2437                                                NaN   \n",
      "2438                                                NaN   \n",
      "2439  [{'domain': {'id': '46', 'name': 'Business Tax...   \n",
      "\n",
      "                    created_at             tweet_id  in_reply_to_user_id  geo  \\\n",
      "0     2024-03-12T20:59:47.000Z  1767656760607719717                  NaN  NaN   \n",
      "1     2024-03-12T20:32:46.000Z  1767649960193913093                  NaN  NaN   \n",
      "2     2024-03-12T20:29:27.000Z  1767649125540008328                  NaN  NaN   \n",
      "3     2024-03-12T20:20:25.000Z  1767646850444648640                  NaN  NaN   \n",
      "4     2024-03-12T20:15:34.000Z  1767645630338584718          233913342.0  NaN   \n",
      "...                        ...                  ...                  ...  ...   \n",
      "2435  2024-02-13T14:19:01.000Z  1757409041905562081                  NaN  NaN   \n",
      "2436  2024-02-13T14:19:00.000Z  1757409037530919300                  NaN  NaN   \n",
      "2437  2024-02-19T06:39:37.000Z  1759467758570619388                  NaN  NaN   \n",
      "2438  2024-02-15T05:36:10.000Z  1758002237065097265                  NaN  NaN   \n",
      "2439  2024-02-20T09:29:20.000Z  1759872858484851152                  NaN  NaN   \n",
      "\n",
      "                                                metrics  total_engagement  \\\n",
      "0     {'retweet_count': 6, 'reply_count': 0, 'like_c...                 6   \n",
      "1     {'retweet_count': 0, 'reply_count': 0, 'like_c...                50   \n",
      "2     {'retweet_count': 0, 'reply_count': 0, 'like_c...                 1   \n",
      "3     {'retweet_count': 4, 'reply_count': 0, 'like_c...                 4   \n",
      "4     {'retweet_count': 0, 'reply_count': 0, 'like_c...                 4   \n",
      "...                                                 ...               ...   \n",
      "2435  {'retweet_count': 0, 'reply_count': 0, 'like_c...                 0   \n",
      "2436  {'retweet_count': 0, 'reply_count': 0, 'like_c...                14   \n",
      "2437  {'retweet_count': 2, 'reply_count': 0, 'like_c...                 2   \n",
      "2438  {'retweet_count': 2, 'reply_count': 0, 'like_c...                 2   \n",
      "2439  {'retweet_count': 0, 'reply_count': 0, 'like_c...                62   \n",
      "\n",
      "      log_engage                                    processed_tweet  \n",
      "0       1.945910  rt appears government yet grasp fully scale e ...  \n",
      "1       3.931826  u department energy new electronics scrap recy...  \n",
      "2       0.693147  keep ucf green 4green planet electronic waste ...  \n",
      "3       1.609438  rt dell tackle e waste giant 3d qr code instal...  \n",
      "4       1.609438  rich country import mountain future e waste ch...  \n",
      "...          ...                                                ...  \n",
      "2435    0.000000  consumer buy product get specific job done fin...  \n",
      "2436    2.708050  miss ep 2 wicked sustainability challenge seri...  \n",
      "2437    1.098612  rt exhibition organized renovated antique furn...  \n",
      "2438    1.098612  rt exhibition organized renovated antique furn...  \n",
      "2439    4.143135  discover wisetek leading charge transforming e...  \n",
      "\n",
      "[2440 rows x 16 columns]>\n"
     ]
    }
   ],
   "source": [
    "# 2. Text Preprocessing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Downloading necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text by lowercasing, removing URLs, mentions, hashtags, punctuation, and applying tokenization and lemmatization.\"\"\"\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs, mentions, and hashtags\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+|@\\w+|#\\w+', '', text)\n",
    "    # Remove non-word characters and tokenize\n",
    "    tokens = word_tokenize(re.sub(r'\\W+', ' ', text))\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "# Apply preprocessing to the tweet column\n",
    "tweets_df['processed_tweet'] = tweets_df['text'].apply(preprocess_text)\n",
    "tweets_df.to_csv('TweetData/combined_tweets_data.csv', index=False)\n",
    "print(tweets_df.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 3**:\n",
    "- **Removing Non-English Tweets, Duplicates, and Advertisements**: This cell removes duplicate tweets, non-English tweets, and promotional tweets based on specific keywords. It prints the number of tweets removed at each step and saves the cleaned DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of tweets: 2440\n",
      "Number of duplicate tweets removed: 0\n",
      "Number of non-English tweets removed: 185\n",
      "Number of promotional tweets removed: 729\n",
      "Number of tweets after cleaning: 1526\n",
      "Sample of cleaned data:\n",
      "                                      public_metrics  \\\n",
      "0  {'retweet_count': 6, 'reply_count': 0, 'like_c...   \n",
      "4  {'retweet_count': 0, 'reply_count': 0, 'like_c...   \n",
      "5  {'retweet_count': 2, 'reply_count': 0, 'like_c...   \n",
      "7  {'retweet_count': 20, 'reply_count': 0, 'like_...   \n",
      "8  {'retweet_count': 2, 'reply_count': 0, 'like_c...   \n",
      "\n",
      "                                                text  conversation_id  \\\n",
      "0  RT @RestartProject: It appears the government ...     1.767657e+18   \n",
      "4  @davidfickling @IEA Rich countries import moun...     1.767438e+18   \n",
      "5  RT @M_Star_Online: Government failing to take ...     1.767644e+18   \n",
      "7  RT @caniravkaria: This may be beginning of E-w...     1.767640e+18   \n",
      "8  RT @ahier: Pulling gold out of e-waste suddenl...     1.767639e+18   \n",
      "\n",
      "    edit_history_tweet_ids lang  \\\n",
      "0  ['1767656760607719717']   en   \n",
      "4  ['1767645630338584718']   en   \n",
      "5  ['1767643949961785639']   en   \n",
      "7  ['1767640057232470519']   en   \n",
      "8  ['1767639365520388344']   en   \n",
      "\n",
      "                                   referenced_tweets     author_id  \\\n",
      "0  [{'type': 'retweeted', 'id': '1767483408089833...  1.333596e+09   \n",
      "4  [{'type': 'replied_to', 'id': '176743802119935...  1.062253e+18   \n",
      "5  [{'type': 'retweeted', 'id': '1767629171155476...  1.278335e+18   \n",
      "7  [{'type': 'retweeted', 'id': '1767577322159489...  8.014157e+17   \n",
      "8  [{'type': 'retweeted', 'id': '1766786578079261...  1.824433e+07   \n",
      "\n",
      "                                 context_annotations  \\\n",
      "0  [{'domain': {'id': '10', 'name': 'Person', 'de...   \n",
      "4  [{'domain': {'id': '30', 'name': 'Entities [En...   \n",
      "5  [{'domain': {'id': '131', 'name': 'Unified Twi...   \n",
      "7                                                NaN   \n",
      "8                                                NaN   \n",
      "\n",
      "                 created_at             tweet_id  in_reply_to_user_id  geo  \\\n",
      "0  2024-03-12T20:59:47.000Z  1767656760607719717                  NaN  NaN   \n",
      "4  2024-03-12T20:15:34.000Z  1767645630338584718          233913342.0  NaN   \n",
      "5  2024-03-12T20:08:53.000Z  1767643949961785639                  NaN  NaN   \n",
      "7  2024-03-12T19:53:25.000Z  1767640057232470519                  NaN  NaN   \n",
      "8  2024-03-12T19:50:40.000Z  1767639365520388344                  NaN  NaN   \n",
      "\n",
      "                                             metrics  total_engagement  \\\n",
      "0  {'retweet_count': 6, 'reply_count': 0, 'like_c...                 6   \n",
      "4  {'retweet_count': 0, 'reply_count': 0, 'like_c...                 4   \n",
      "5  {'retweet_count': 2, 'reply_count': 0, 'like_c...                 2   \n",
      "7  {'retweet_count': 20, 'reply_count': 0, 'like_...                20   \n",
      "8  {'retweet_count': 2, 'reply_count': 0, 'like_c...                 2   \n",
      "\n",
      "   log_engage                                    processed_tweet  \n",
      "0    1.945910  rt appears government yet grasp fully scale e ...  \n",
      "4    1.609438  rich country import mountain future e waste ch...  \n",
      "5    1.098612  rt government failing take britain e waste tsu...  \n",
      "7    3.044522  rt may beginning e waste theme miss theme kach...  \n",
      "8    1.098612  rt pulling gold e waste suddenly becomes super...  \n"
     ]
    }
   ],
   "source": [
    "# 3. Removing non-English tweets, duplicates, and any that might be advertisements\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming tweets_df is the DataFrame already loaded from the previous steps\n",
    "\n",
    "# Initial number of tweets\n",
    "initial_count = tweets_df.shape[0]\n",
    "\n",
    "# Step 1: Remove Duplicate Tweets\n",
    "tweets_df = tweets_df.drop_duplicates(subset='tweet_id', keep='first')\n",
    "after_duplicates_count = tweets_df.shape[0]\n",
    "duplicates_removed = initial_count - after_duplicates_count\n",
    "\n",
    "# Step 2: Remove Non-English Tweets\n",
    "tweets_df = tweets_df[tweets_df['lang'] == 'en']\n",
    "after_non_english_count = tweets_df.shape[0]\n",
    "non_english_removed = after_duplicates_count - after_non_english_count\n",
    "\n",
    "# Step 3: Remove Advertising/Promotional Tweets\n",
    "\n",
    "\n",
    "def is_promotional(text):\n",
    "    \"\"\"Identify promotional tweets based on common promotional keywords.\"\"\"\n",
    "    promotional_keywords = [\n",
    "        'buy now', 'free', 'discount', 'offer', 'sale', 'shop', 'promotion',\n",
    "        'sponsored', 'advertisement', 'ad', 'click here', 'visit our site',\n",
    "        'subscribe', 'check out', 'limited time'\n",
    "    ]\n",
    "    for keyword in promotional_keywords:\n",
    "        if keyword in text.lower():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Apply the is_promotional function to filter out promotional tweets\n",
    "tweets_df = tweets_df[~tweets_df['text'].apply(is_promotional)]\n",
    "after_promotional_count = tweets_df.shape[0]\n",
    "promotional_removed = after_non_english_count - after_promotional_count\n",
    "\n",
    "tweets_df.to_csv('TweetData/combined_tweets_data.csv', index=False)\n",
    "\n",
    "# Display the number of tweets removed at each step\n",
    "print(f\"Initial number of tweets: {initial_count}\")\n",
    "print(f\"Number of duplicate tweets removed: {duplicates_removed}\")\n",
    "print(f\"Number of non-English tweets removed: {non_english_removed}\")\n",
    "print(f\"Number of promotional tweets removed: {promotional_removed}\")\n",
    "print(f\"Number of tweets after cleaning: {tweets_df.shape[0]}\")\n",
    "\n",
    "# Display a sample of the cleaned data\n",
    "print(\"Sample of cleaned data:\")\n",
    "print(tweets_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 4**:\n",
    "- **Extracting Metrics and Log Normalization**: This cell defines a function to extract metrics from JSON strings and handle possible errors. It applies this function to the 'public_metrics' column, sums the metrics into a new column, and log normalizes the total engagement. The updated DataFrame is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Extracting Metrics and Log Normalization\n",
    "import json\n",
    "\n",
    "def extract_metrics(json_str):\n",
    "    \"\"\"Extract metrics from JSON string and handle possible errors.\"\"\"\n",
    "    keys = ['retweet_count', 'reply_count', 'like_count',\n",
    "            'quote_count', 'bookmark_count', 'impression_count']\n",
    "    try:\n",
    "        metrics = json.loads(json_str.replace(\"'\", '\"'))\n",
    "        # Ensure all keys are present, defaulting to 0 if not\n",
    "        return {key: metrics.get(key, 0) for key in keys}\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}, input: {json_str}\")\n",
    "        return {key: 0 for key in keys}\n",
    "\n",
    "\n",
    "# Apply the extract_metrics function\n",
    "tweets_df['metrics'] = tweets_df['public_metrics'].apply(extract_metrics)\n",
    "\n",
    "# Sum the metrics into a new column\n",
    "tweets_df['total_engagement'] = tweets_df['metrics'].apply(\n",
    "    lambda x: sum(x.values()))\n",
    "\n",
    "# Log normalize the total engagement\n",
    "tweets_df['log_engage'] = np.log1p(tweets_df['total_engagement'])\n",
    "\n",
    "tweets_df.to_csv('TweetData/combined_tweets_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 5**:\n",
    "- **Generate Embeddings**: This cell defines a custom Dataset class for handling text data and a function to generate embeddings using a pre-trained RoBERTa model. It loads the processed dataset, verifies columns, loads the RoBERTa model and tokenizer, generates embeddings, and saves them to a .npy file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Using device: cpu\n",
      "Loading RoBERTa model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n",
      "Embeddings generation complete.\n",
      "Saving embeddings...\n",
      "Embeddings saved successfully to TweetData/roberta_tweets_embeddings.npy.\n",
      "Process completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# 5. Generate embeddings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "\n",
    "# Ensure Numpy prints arrays completely\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for handling text data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, texts):\n",
    "        \"\"\"\n",
    "        Initialize with a list of texts.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the length of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return the text at the given index.\n",
    "        \"\"\"\n",
    "        return self.texts[idx]\n",
    "    \n",
    "\n",
    "def get_embeddings(model, tokenizer, texts, batch_size=16, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of texts using a pre-trained RoBERTa model.\n",
    "    \n",
    "    Args:\n",
    "    - model: Pre-trained RoBERTa model.\n",
    "    - tokenizer: Corresponding tokenizer.\n",
    "    - texts: List of texts to process.\n",
    "    - batch_size: Batch size for processing.\n",
    "    - device: Device to run the model on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Array of embeddings.\n",
    "    \"\"\"\n",
    "    dataset = TextDataset(texts)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    model = model.to(device)\n",
    "    all_embeddings = []\n",
    "\n",
    "    for batch_texts in data_loader:\n",
    "        try:\n",
    "            inputs = tokenizer(batch_texts, return_tensors=\"pt\",\n",
    "                               padding=True, truncation=True, max_length=512)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state.mean(\n",
    "                dim=1).detach().cpu().numpy()\n",
    "            all_embeddings.append(embeddings)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch: {e}\")\n",
    "\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "\n",
    "def save_embeddings(embeddings, file_name):\n",
    "    \"\"\"\n",
    "    Save embeddings to a .npy file for efficient loading and use in PyG.\n",
    "    \n",
    "    Args:\n",
    "    - embeddings: Embeddings to save.\n",
    "    - file_name: Name of the file to save the embeddings.\n",
    "    \"\"\"\n",
    "    np.save(file_name, embeddings)\n",
    "    print(f\"Embeddings saved successfully to {file_name}.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load preprocessed dataset\n",
    "        print(\"Loading dataset...\")\n",
    "        tweets_df = pd.read_csv('TweetData/combined_tweets_data.csv')\n",
    "\n",
    "        # Verify columns\n",
    "        if 'processed_tweet' not in tweets_df.columns:\n",
    "            raise ValueError(\"Processed tweet column not found in the dataset.\")\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "        # Load RoBERTa model and tokenizer\n",
    "        print(\"Loading RoBERTa model and tokenizer...\")\n",
    "        model = RobertaModel.from_pretrained('roberta-base')\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "        # Get the list of processed texts\n",
    "        texts = tweets_df['processed_tweet'].tolist()\n",
    "\n",
    "        # Generate embeddings\n",
    "        print(\"Generating embeddings...\")\n",
    "        embeddings = get_embeddings(model, tokenizer, texts, device=device)\n",
    "        print(\"Embeddings generation complete.\")\n",
    "\n",
    "        # Save embeddings to .npy file\n",
    "        embeddings_file = \"TweetData/roberta_tweets_embeddings.npy\"\n",
    "        print(\"Saving embeddings...\")\n",
    "        save_embeddings(embeddings, embeddings_file)\n",
    "        print(\"Process completed successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "'''import numpy as np\n",
    "embeddings = np.load(\"TweetData/roberta_tweets_embeddings.npy\")\n",
    "# to use embeddings for cosine similarity calculations\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 6**:\n",
    "- **Identify and Remove Duplicate Tweets Using Embeddings**: This cell loads the embeddings and the original dataset, ensures they have the same length, computes pairwise cosine similarity, identifies duplicate tweets based on a similarity threshold, removes duplicates from the DataFrame and embeddings array, and saves the cleaned data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify and remove duplicate tweets using the embeddings, we can follow these steps:\n",
    "\n",
    "1. **Loading**: Loaded embeddings and the original dataset.\n",
    "2. **Similarity Calculation**: Computed pairwise cosine similarity.\n",
    "3. **Thresholding**: Defined a threshold to identify duplicates.\n",
    "4. **Removing Duplicates**: Identified and removed duplicate tweets based on the similarity threshold, saving the cleaned data.\n",
    "\n",
    "By following this process, we ensure that we retain unique tweets in the dataset, which will help in improving the accuracy and efficiency of the subsequent analysis, especially in a graph-based model where redundancy can lead to skewed results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 352 duplicate tweets. Cleaned data and embeddings saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the embeddings and the original dataset\n",
    "embeddings = np.load(\"TweetData/roberta_tweets_embeddings.npy\")\n",
    "tweets_df = pd.read_csv('TweetData/combined_tweets_data.csv')\n",
    "\n",
    "# Ensure the embeddings and the DataFrame have the same length\n",
    "assert len(embeddings) == len(\n",
    "    tweets_df), \"Mismatch between embeddings and DataFrame length.\"\n",
    "\n",
    "# Compute pairwise cosine similarity\n",
    "cosine_similarities = 1 - cdist(embeddings, embeddings, metric='cosine')\n",
    "\n",
    "# Define the similarity threshold for considering tweets as duplicates\n",
    "similarity_threshold = 0.99999999\n",
    "\n",
    "\n",
    "def identify_duplicates(similarity_matrix, threshold):\n",
    "    \"\"\"\n",
    "    Identify duplicates in the similarity matrix based on the given threshold.\n",
    "    \n",
    "    Args:\n",
    "    - similarity_matrix: Pairwise cosine similarity matrix.\n",
    "    - threshold: Cosine similarity threshold to consider tweets as duplicates.\n",
    "    \n",
    "    Returns:\n",
    "    - List of indices of duplicate tweets to be removed.\n",
    "    \"\"\"\n",
    "    num_tweets = similarity_matrix.shape[0]\n",
    "    duplicates = set()\n",
    "\n",
    "    for i in range(num_tweets):\n",
    "        for j in range(i + 1, num_tweets):\n",
    "            if similarity_matrix[i, j] > threshold:\n",
    "                duplicates.add(j)\n",
    "\n",
    "    return list(duplicates)\n",
    "\n",
    "\n",
    "# Identify duplicate tweet indices\n",
    "duplicate_indices = identify_duplicates(\n",
    "    cosine_similarities, similarity_threshold)\n",
    "\n",
    "# Remove duplicates from the DataFrame\n",
    "tweets_df_no_duplicates = tweets_df.drop(\n",
    "    index=duplicate_indices).reset_index(drop=True)\n",
    "\n",
    "# Remove duplicates from the embeddings array\n",
    "embeddings_no_duplicates = np.delete(embeddings, duplicate_indices, axis=0)\n",
    "\n",
    "# Save the cleaned DataFrame\n",
    "tweets_df_no_duplicates.to_csv(\n",
    "    'TweetData/combined_tweets_no_duplicates.csv', index=False)\n",
    "\n",
    "# Save the updated embeddings array to a new file\n",
    "np.save(\"TweetData/roberta_tweets_embeddings_no_duplicates.npy\",\n",
    "        embeddings_no_duplicates)\n",
    "\n",
    "print(f\"Removed {len(duplicate_indices)} duplicate tweets. Cleaned data and embeddings saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 7**:\n",
    "- **Load and Apply Sentiment Analysis Model**: This cell loads a pre-trained sentiment analysis model and tokenizer, performs sentiment analysis on the processed tweet texts, and saves the updated DataFrame with sentiment probabilities and the tensor containing sentiment probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cardiffnlp/twitter-roberta-base-sentiment` model is a pre-trained sentiment analysis model specifically fine-tuned on Twitter data. When you use this model to analyze text, it outputs probabilities for three sentiment classes: positive, neutral, and negative.\n",
    "\n",
    "### How the Model Responds\n",
    "\n",
    "When you pass a tweet or text to the model, it tokenizes the text and feeds it through the RoBERTa architecture. The model then outputs a tensor with probabilities for each sentiment class. Here's how you can use the model and interpret its response:\n",
    "\n",
    "### Interpreting the Response\n",
    "\n",
    "1. **Sentiment Labels**: The model provides a prediction for one of three sentiment labels:\n",
    "    - **Negative**: Indicates a negative sentiment in the text.\n",
    "    - **Neutral**: Indicates a neutral sentiment in the text.\n",
    "    - **Positive**: Indicates a positive sentiment in the text.\n",
    "\n",
    "2. **Probabilities**: The model outputs a list of probabilities corresponding to each sentiment class. These probabilities indicate the model's confidence in each class:\n",
    "    - **Probability Distribution**: The probabilities for the negative, neutral, and positive classes will sum to 1. For instance, if the probabilities are `[0.1, 0.3, 0.6]`, it means the model assigns a 10% chance to negative sentiment, 30% to neutral, and 60% to positive.\n",
    "\n",
    "### Example Interpretation\n",
    "\n",
    "Given the text \"I love using the new sentiment analysis model!\", suppose the model returns the following probabilities:\n",
    "\n",
    "```python\n",
    "Probabilities: [0.05, 0.1, 0.85]\n",
    "```\n",
    "\n",
    "This output means:\n",
    "- **Negative**: 5% chance\n",
    "- **Neutral**: 10% chance\n",
    "- **Positive**: 85% chance\n",
    "\n",
    "The highest probability is for the positive class, so the predicted sentiment is **positive**. This indicates that the model is highly confident that the text expresses a positive sentiment.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Model Response**: The model outputs a probability distribution over three sentiment classes: negative, neutral, and positive.\n",
    "- **Interpretation**: The class with the highest probability is the predicted sentiment. The probabilities provide a measure of the model's confidence in each sentiment class.\n",
    "- **Application**: This approach can be used to analyze the sentiment of tweets or any other short text inputs, making it particularly useful for social media analysis.\n",
    "\n",
    "By using the `cardiffnlp/twitter-roberta-base-sentiment` model, you can accurately and efficiently gauge the sentiment of tweets, which can be valuable for various applications, such as monitoring public opinion, tracking brand sentiment, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment model and tokenizer loaded successfully.\n",
      "Loaded 1174 tweets from TweetData/combined_tweets_no_duplicates.csv.\n",
      "Using device: cpu\n",
      "Sentiment analysis applied to all tweets.\n",
      "Updated DataFrame saved to TweetData/roberta_tweets_sentiments.csv.\n",
      "Tensor saved to TweetData/roberta_tweets_sentiments_tensor.pt.\n",
      "Verification successful. First few rows of TweetData/roberta_tweets_sentiments.csv:\n",
      "                                      public_metrics  \\\n",
      "0  {'retweet_count': 6, 'reply_count': 0, 'like_c...   \n",
      "1  {'retweet_count': 0, 'reply_count': 0, 'like_c...   \n",
      "2  {'retweet_count': 2, 'reply_count': 0, 'like_c...   \n",
      "3  {'retweet_count': 20, 'reply_count': 0, 'like_...   \n",
      "4  {'retweet_count': 2, 'reply_count': 0, 'like_c...   \n",
      "\n",
      "                                                text  conversation_id  \\\n",
      "0  RT @RestartProject: It appears the government ...     1.767657e+18   \n",
      "1  @davidfickling @IEA Rich countries import moun...     1.767438e+18   \n",
      "2  RT @M_Star_Online: Government failing to take ...     1.767644e+18   \n",
      "3  RT @caniravkaria: This may be beginning of E-w...     1.767640e+18   \n",
      "4  RT @ahier: Pulling gold out of e-waste suddenl...     1.767639e+18   \n",
      "\n",
      "    edit_history_tweet_ids lang  \\\n",
      "0  ['1767656760607719717']   en   \n",
      "1  ['1767645630338584718']   en   \n",
      "2  ['1767643949961785639']   en   \n",
      "3  ['1767640057232470519']   en   \n",
      "4  ['1767639365520388344']   en   \n",
      "\n",
      "                                   referenced_tweets     author_id  \\\n",
      "0  [{'type': 'retweeted', 'id': '1767483408089833...  1.333596e+09   \n",
      "1  [{'type': 'replied_to', 'id': '176743802119935...  1.062253e+18   \n",
      "2  [{'type': 'retweeted', 'id': '1767629171155476...  1.278335e+18   \n",
      "3  [{'type': 'retweeted', 'id': '1767577322159489...  8.014157e+17   \n",
      "4  [{'type': 'retweeted', 'id': '1766786578079261...  1.824433e+07   \n",
      "\n",
      "                                 context_annotations  \\\n",
      "0  [{'domain': {'id': '10', 'name': 'Person', 'de...   \n",
      "1  [{'domain': {'id': '30', 'name': 'Entities [En...   \n",
      "2  [{'domain': {'id': '131', 'name': 'Unified Twi...   \n",
      "3                                                NaN   \n",
      "4                                                NaN   \n",
      "\n",
      "                 created_at             tweet_id  in_reply_to_user_id  geo  \\\n",
      "0  2024-03-12T20:59:47.000Z  1767656760607719717                  NaN  NaN   \n",
      "1  2024-03-12T20:15:34.000Z  1767645630338584718          233913342.0  NaN   \n",
      "2  2024-03-12T20:08:53.000Z  1767643949961785639                  NaN  NaN   \n",
      "3  2024-03-12T19:53:25.000Z  1767640057232470519                  NaN  NaN   \n",
      "4  2024-03-12T19:50:40.000Z  1767639365520388344                  NaN  NaN   \n",
      "\n",
      "                                             metrics  total_engagement  \\\n",
      "0  {'retweet_count': 6, 'reply_count': 0, 'like_c...                 6   \n",
      "1  {'retweet_count': 0, 'reply_count': 0, 'like_c...                 4   \n",
      "2  {'retweet_count': 2, 'reply_count': 0, 'like_c...                 2   \n",
      "3  {'retweet_count': 20, 'reply_count': 0, 'like_...                20   \n",
      "4  {'retweet_count': 2, 'reply_count': 0, 'like_c...                 2   \n",
      "\n",
      "   log_engage                                    processed_tweet  positive  \\\n",
      "0    1.945910  rt appears government yet grasp fully scale e ...  0.321021   \n",
      "1    1.609438  rich country import mountain future e waste ch...  0.452236   \n",
      "2    1.098612  rt government failing take britain e waste tsu...  0.872248   \n",
      "3    3.044522  rt may beginning e waste theme miss theme kach...  0.266101   \n",
      "4    1.098612  rt pulling gold e waste suddenly becomes super...  0.076413   \n",
      "\n",
      "    neutral  negative  \n",
      "0  0.646973  0.032006  \n",
      "1  0.497398  0.050366  \n",
      "2  0.120716  0.007036  \n",
      "3  0.707849  0.026050  \n",
      "4  0.463236  0.460351  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_sentiment_model():\n",
    "    \"\"\"\n",
    "    Load the sentiment analysis model and tokenizer.\n",
    "    \n",
    "    Returns:\n",
    "    - tokenizer: Pre-trained tokenizer for sentiment analysis.\n",
    "    - model: Pre-trained sentiment analysis model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "        print(\"Sentiment model and tokenizer loaded successfully.\")\n",
    "        return tokenizer, model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sentiment model or tokenizer: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def sentiment_analysis(texts, tokenizer, model, batch_size=16, device='cpu'):\n",
    "    \"\"\"\n",
    "    Perform sentiment analysis on a list of texts using a pre-trained model.\n",
    "    \n",
    "    Args:\n",
    "    - texts: List of texts to analyze.\n",
    "    - tokenizer: Pre-trained tokenizer.\n",
    "    - model: Pre-trained sentiment analysis model.\n",
    "    - batch_size: Batch size for processing.\n",
    "    - device: Device to run the model on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Tensor containing sentiment probabilities for each text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = model.to(device)\n",
    "        all_scores = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            encoded_input = tokenizer(\n",
    "                batch_texts, return_tensors='pt', truncation=True, max_length=512, padding=True)\n",
    "            encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "            with torch.no_grad():\n",
    "                output = model(**encoded_input)\n",
    "            scores = torch.nn.functional.softmax(\n",
    "                output.logits, dim=-1).cpu()\n",
    "            all_scores.append(scores)\n",
    "        return torch.cat(all_scores, dim=0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error performing sentiment analysis: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def load_tweets(file_path):\n",
    "    \"\"\"\n",
    "    Load the tweets and embeddings from a CSV file.\n",
    "    \n",
    "    Args:\n",
    "    - file_path: Path to the CSV file containing the tweets and embeddings.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame containing the tweets and their embeddings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tweets_df = pd.read_csv(file_path)\n",
    "        print(f\"Loaded {len(tweets_df)} tweets from {file_path}.\")\n",
    "        return tweets_df\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading file: {e}\")\n",
    "        raise\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"Error parsing file: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def apply_sentiment_analysis(tweets_df, tokenizer, model, batch_size=16, device='cpu'):\n",
    "    \"\"\"\n",
    "    Apply sentiment analysis to each tweet in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "    - tweets_df: DataFrame containing the tweets.\n",
    "    - tokenizer: Pre-trained tokenizer.\n",
    "    - model: Pre-trained sentiment analysis model.\n",
    "    - batch_size: Batch size for processing.\n",
    "    - device: Device to run the model on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Updated DataFrame with sentiment probabilities.\n",
    "    - torch.Tensor: Tensor containing sentiment probabilities.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        texts = tweets_df['processed_tweet'].tolist()\n",
    "        sentiments = sentiment_analysis(\n",
    "            texts, tokenizer, model, batch_size, device)\n",
    "        sentiments_df = pd.DataFrame(\n",
    "            sentiments.numpy(), columns=['positive', 'neutral', 'negative'])\n",
    "        tweets_df = pd.concat([tweets_df, sentiments_df], axis=1)\n",
    "        print(\"Sentiment analysis applied to all tweets.\")\n",
    "        return tweets_df, sentiments\n",
    "    except Exception as e:\n",
    "        print(f\"Error applying sentiment analysis: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def save_updated_dataframe(tweets_df, file_path):\n",
    "    \"\"\"\n",
    "    Save the updated DataFrame with sentiment probabilities to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "    - tweets_df: DataFrame containing the updated tweets.\n",
    "    - file_path: Path to save the updated CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tweets_df.to_csv(file_path, index=False)\n",
    "        print(f\"Updated DataFrame saved to {file_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving updated DataFrame: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def save_tensor(tensor, file_path):\n",
    "    \"\"\"\n",
    "    Save the tensor to a file.\n",
    "    \n",
    "    Args:\n",
    "    - tensor: Tensor to save.\n",
    "    - file_path: Path to save the tensor file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        torch.save(tensor, file_path)\n",
    "        print(f\"Tensor saved to {file_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving tensor: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def verify_saved_file(file_path):\n",
    "    \"\"\"\n",
    "    Verify the integrity of the saved CSV file by loading it and checking the first few rows.\n",
    "    \n",
    "    Args:\n",
    "    - file_path: Path to the saved CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Verification successful. First few rows of {file_path}:\")\n",
    "        print(df.head())\n",
    "    except Exception as e:\n",
    "        print(f\"Error verifying saved file: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load the sentiment analysis model and tokenizer\n",
    "        tokenizer, model = load_sentiment_model()\n",
    "\n",
    "        # Load the tweets DataFrame\n",
    "        tweets_df = load_tweets('TweetData/combined_tweets_no_duplicates.csv')\n",
    "\n",
    "        # Determine device\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "        # Apply sentiment analysis to the DataFrame\n",
    "        tweets_df, sentiments_tensor = apply_sentiment_analysis(\n",
    "            tweets_df, tokenizer, model, device=device)\n",
    "\n",
    "        # Save the updated DataFrame\n",
    "        save_updated_dataframe(\n",
    "            tweets_df, 'TweetData/roberta_tweets_sentiments.csv')\n",
    "\n",
    "        # Save the tensor containing sentiment probabilities\n",
    "        save_tensor(sentiments_tensor,\n",
    "                    'TweetData/roberta_tweets_sentiments_tensor.pt')\n",
    "\n",
    "        # Verify the saved file\n",
    "        verify_saved_file('TweetData/roberta_tweets_sentiments.csv')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in the main execution block: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 8**:\n",
    "- **PCA and Normalization**: This cell applies dimensionality reduction using PCA to the embeddings and normalizes them using StandardScaler. It retains 95% variance, saves the updated embeddings, updates the DataFrame with PCA embeddings, and saves the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA and normalization:\n",
    "Apply dimensionality reduction using Principal Component Analysis (PCA), and normalization. This will help to spread out the embeddings in the feature space, reducing the likelihood of high similarity among embeddings and making connections more meaningful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated tweet embeddings saved to 'TweetData/roberta_tweets_embeddings_pca.npy'.\n",
      "DataFrame with PCA embeddings saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the tweet embeddings and the original dataset\n",
    "tweet_embeddings = np.load(\n",
    "    \"TweetData/roberta_tweets_embeddings_no_duplicates.npy\")\n",
    "tweets_df = pd.read_csv('TweetData/roberta_tweets_sentiments.csv')\n",
    "\n",
    "# Ensure the embeddings and the DataFrame have the same length\n",
    "assert len(tweet_embeddings) == len(\n",
    "    tweets_df), \"Mismatch between tweet embeddings and DataFrame length.\"\n",
    "\n",
    "# Standardize the embeddings\n",
    "scaler = StandardScaler()\n",
    "tweet_embeddings_scaled = scaler.fit_transform(tweet_embeddings)\n",
    "\n",
    "# Apply PCA to reduce dimensionality while retaining 95% variance\n",
    "pca = PCA(n_components=0.95)\n",
    "tweet_embeddings_pca = pca.fit_transform(tweet_embeddings_scaled)\n",
    "\n",
    "# Save the updated embeddings\n",
    "np.save(\"TweetData/roberta_tweets_embeddings_pca.npy\", tweet_embeddings_pca)\n",
    "print(f\"Updated tweet embeddings saved to 'TweetData/roberta_tweets_embeddings_pca.npy'.\")\n",
    "\n",
    "# Update the DataFrame with PCA embeddings\n",
    "tweets_df['pca_embeddings'] = list(tweet_embeddings_pca)\n",
    "tweets_df.to_csv('TweetData/updated_tweets_with_pca.csv', index=False)\n",
    "\n",
    "print(\"DataFrame with PCA embeddings saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 9**:\n",
    "- **Drop Unnecessary Columns and Convert Date Format**: This cell drops specified columns from the DataFrame, converts the 'created_at' column to datetime format, and saves the modified DataFrame. It prints the DataFrame for troubleshooting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to TweetData/tweets.csv\n",
      "   conversation_id  created_at             tweet_id  total_engagement  \\\n",
      "0     1.767657e+18  2024-03-12  1767656760607719717                 6   \n",
      "1     1.767438e+18  2024-03-12  1767645630338584718                 4   \n",
      "2     1.767644e+18  2024-03-12  1767643949961785639                 2   \n",
      "3     1.767640e+18  2024-03-12  1767640057232470519                20   \n",
      "4     1.767639e+18  2024-03-12  1767639365520388344                 2   \n",
      "\n",
      "   log_engage  positive   neutral  negative  \\\n",
      "0    1.945910  0.321021  0.646973  0.032006   \n",
      "1    1.609438  0.452236  0.497398  0.050366   \n",
      "2    1.098612  0.872248  0.120716  0.007036   \n",
      "3    3.044522  0.266101  0.707849  0.026050   \n",
      "4    1.098612  0.076413  0.463236  0.460351   \n",
      "\n",
      "                                      pca_embeddings  \n",
      "0  [ 1.23029685e+00  2.54166198e+00 -5.97019196e+...  \n",
      "1  [-3.64204764e+00 -5.98940229e+00 -2.60534310e+...  \n",
      "2  [ 2.38513160e+00  5.06190443e+00  2.03545779e-...  \n",
      "3  [-1.23132029e+01 -2.70899820e+00  3.08240247e+...  \n",
      "4  [ 1.92101359e+00  6.21080351e+00 -1.63610506e+...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "tweets_df = pd.read_csv('TweetData/updated_tweets_with_pca.csv')\n",
    "\n",
    "# Drop the specified columns\n",
    "columns_to_drop = ['public_metrics', 'text', 'edit_history_tweet_ids', 'lang',\n",
    "                   'referenced_tweets', 'in_reply_to_user_id', 'author_id', 'context_annotations',\n",
    "                   'geo', 'metrics', 'processed_tweet']\n",
    "tweets_df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Convert 'created_at' column to datetime format and focus on month, day, and year\n",
    "tweets_df['created_at'] = pd.to_datetime(\n",
    "    tweets_df['created_at']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Save the modified DataFrame\n",
    "output_path = 'TweetData/tweets.csv'\n",
    "tweets_df.to_csv(output_path, index=False)\n",
    "\n",
    "# Output for troubleshooting\n",
    "print(f\"DataFrame saved to {output_path}\")\n",
    "print(tweets_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 10**:\n",
    "- **Convert Date to Unix Timestamp**: This cell converts the 'created_at' column to Unix timestamp format and saves the updated DataFrame. It prints the DataFrame and its data types for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to 'TweetData/final_tweets.csv'\n",
      "   conversation_id  created_at             tweet_id  total_engagement  \\\n",
      "0     1.767657e+18  1710201600  1767656760607719717                 6   \n",
      "1     1.767438e+18  1710201600  1767645630338584718                 4   \n",
      "2     1.767644e+18  1710201600  1767643949961785639                 2   \n",
      "3     1.767640e+18  1710201600  1767640057232470519                20   \n",
      "4     1.767639e+18  1710201600  1767639365520388344                 2   \n",
      "\n",
      "   log_engage  positive   neutral  negative  \\\n",
      "0    1.945910  0.321021  0.646973  0.032006   \n",
      "1    1.609438  0.452236  0.497398  0.050366   \n",
      "2    1.098612  0.872248  0.120716  0.007036   \n",
      "3    3.044522  0.266101  0.707849  0.026050   \n",
      "4    1.098612  0.076413  0.463236  0.460351   \n",
      "\n",
      "                                      pca_embeddings  \n",
      "0  [ 1.23029685e+00  2.54166198e+00 -5.97019196e+...  \n",
      "1  [-3.64204764e+00 -5.98940229e+00 -2.60534310e+...  \n",
      "2  [ 2.38513160e+00  5.06190443e+00  2.03545779e-...  \n",
      "3  [-1.23132029e+01 -2.70899820e+00  3.08240247e+...  \n",
      "4  [ 1.92101359e+00  6.21080351e+00 -1.63610506e+...  \n",
      "conversation_id     float64\n",
      "created_at            int64\n",
      "tweet_id              int64\n",
      "total_engagement      int64\n",
      "log_engage          float64\n",
      "positive            float64\n",
      "neutral             float64\n",
      "negative            float64\n",
      "pca_embeddings       object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "tweets_df = pd.read_csv('TweetData/tweets.csv')\n",
    "\n",
    "# Convert 'created_at' to datetime and then to int64\n",
    "tweets_df['created_at'] = pd.to_datetime(tweets_df['created_at'])\n",
    "tweets_df['created_at'] = tweets_df['created_at'].astype(\n",
    "    'int64') // 10**9  # Convert to Unix timestamp in seconds\n",
    "\n",
    "# Save the updated DataFrame\n",
    "tweets_df.to_csv('TweetData/final_tweets.csv', index=False)\n",
    "\n",
    "# Output for verification\n",
    "print(f\"DataFrame saved to 'TweetData/final_tweets.csv'\")\n",
    "print(tweets_df.head())\n",
    "print(tweets_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 11**:\n",
    "- **Final Data Verification**: This cell displays data types, summary statistics for numerical columns, identifies missing values, displays the first few rows of the DataFrame, and prints the total number of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Types:\n",
      "conversation_id     float64\n",
      "created_at            int64\n",
      "tweet_id              int64\n",
      "total_engagement      int64\n",
      "log_engage          float64\n",
      "positive            float64\n",
      "neutral             float64\n",
      "negative            float64\n",
      "pca_embeddings       object\n",
      "dtype: object\n",
      "\n",
      "\n",
      "Summary Statistics:\n",
      "       conversation_id    created_at      tweet_id  total_engagement  \\\n",
      "count     1.144000e+03  1.174000e+03  1.174000e+03       1174.000000   \n",
      "mean      1.762844e+18  1.709111e+09  1.762955e+18        381.459114   \n",
      "std       5.444208e+15  9.344493e+05  3.920997e+15       3782.488004   \n",
      "min       1.634546e+18  1.707782e+09  1.757409e+18          0.000000   \n",
      "25%       1.758884e+18  1.708128e+09  1.758851e+18          4.000000   \n",
      "50%       1.765360e+18  1.709683e+09  1.765383e+18         14.000000   \n",
      "75%       1.766548e+18  1.710007e+09  1.766610e+18         54.750000   \n",
      "max       1.767657e+18  1.710202e+09  1.767657e+18      78533.000000   \n",
      "\n",
      "        log_engage     positive      neutral     negative  \n",
      "count  1174.000000  1174.000000  1174.000000  1174.000000  \n",
      "mean      2.971308     0.274240     0.491569     0.234191  \n",
      "std       1.844498     0.294906     0.253757     0.281904  \n",
      "min       0.000000     0.001045     0.015355     0.002278  \n",
      "25%       1.609438     0.025224     0.272538     0.030944  \n",
      "50%       2.708050     0.129900     0.492878     0.080680  \n",
      "75%       4.020847     0.499592     0.706806     0.396119  \n",
      "max      11.271287     0.981943     0.942477     0.976502  \n",
      "\n",
      "\n",
      "Missing Values:\n",
      "conversation_id     30\n",
      "created_at           0\n",
      "tweet_id             0\n",
      "total_engagement     0\n",
      "log_engage           0\n",
      "positive             0\n",
      "neutral              0\n",
      "negative             0\n",
      "pca_embeddings       0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Data Format (first few rows):\n",
      "   conversation_id  created_at             tweet_id  total_engagement  \\\n",
      "0     1.767657e+18  1710201600  1767656760607719717                 6   \n",
      "1     1.767438e+18  1710201600  1767645630338584718                 4   \n",
      "2     1.767644e+18  1710201600  1767643949961785639                 2   \n",
      "3     1.767640e+18  1710201600  1767640057232470519                20   \n",
      "4     1.767639e+18  1710201600  1767639365520388344                 2   \n",
      "\n",
      "   log_engage  positive   neutral  negative  \\\n",
      "0    1.945910  0.321021  0.646973  0.032006   \n",
      "1    1.609438  0.452236  0.497398  0.050366   \n",
      "2    1.098612  0.872248  0.120716  0.007036   \n",
      "3    3.044522  0.266101  0.707849  0.026050   \n",
      "4    1.098612  0.076413  0.463236  0.460351   \n",
      "\n",
      "                                      pca_embeddings  \n",
      "0  [ 1.23029685e+00  2.54166198e+00 -5.97019196e+...  \n",
      "1  [-3.64204764e+00 -5.98940229e+00 -2.60534310e+...  \n",
      "2  [ 2.38513160e+00  5.06190443e+00  2.03545779e-...  \n",
      "3  [-1.23132029e+01 -2.70899820e+00  3.08240247e+...  \n",
      "4  [ 1.92101359e+00  6.21080351e+00 -1.63610506e+...  \n",
      "Number of tweets: 1174\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "tweets_df = pd.read_csv('TweetData/final_tweets.csv')\n",
    "\n",
    "# Display data types\n",
    "print(\"Data Types:\")\n",
    "print(tweets_df.dtypes)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Display summary statistics for numerical columns\n",
    "print(\"Summary Statistics:\")\n",
    "print(tweets_df.describe())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Identify missing values\n",
    "print(\"Missing Values:\")\n",
    "missing_values = tweets_df.isnull().sum()\n",
    "print(missing_values)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Display the first few rows of the dataframe to understand the data format\n",
    "print(\"Data Format (first few rows):\")\n",
    "print(tweets_df.head())\n",
    "\n",
    "print(f\"Number of tweets: {tweets_df.shape[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
